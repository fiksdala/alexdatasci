{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Model stacking/blending\n",
    "\n",
    "In parts 1-4 of this binary classification series, I saved a total of 8 models predicting history of high blood pressure (4 that accounted for class imbalance, 4 that did not). One way to improve final predictions is to use ensembling techniques. In this post I'll explore a few stacking/blending approaches:\n",
    "\n",
    "- Simple majority vote\n",
    "- XGBoost using model predictions as features\n",
    "- XGBoost using original training data + model predictions as features\n",
    "\n",
    "First, I'll import the models and retrieve both class and probability predictions (not including SVM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/alex/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Train/Val/vars\n",
    "x_train = pickle.load( open( \"x_train.pickle\", \"rb\" ) )\n",
    "x_val = pickle.load( open( \"x_val.pickle\", \"rb\" ) )\n",
    "x_test = pickle.load( open( \"x_test.pickle\", \"rb\" ) )\n",
    "y_train = pickle.load( open( \"y_train.pickle\", \"rb\" ) )\n",
    "y_val = pickle.load( open( \"y_val.pickle\", \"rb\" ) )\n",
    "y_test = pickle.load( open( \"y_test.pickle\", \"rb\" ) )\n",
    "keep_vars10 = pickle.load( open( \"keep_vars10.pickle\", \"rb\" ) )\n",
    "\n",
    "\n",
    "# Make dicts of models\n",
    "models = {'xgb_bal':pickle.load(open('xgb_bal.pickle','rb')),\n",
    "          'xgb_unbal':pickle.load(open('xgb_unbal.pickle','rb')),\n",
    "          'svc_bal':pickle.load(open('svc_bal.pickle','rb')),\n",
    "          'svc_unbal':pickle.load(open('svc_unbal.pickle','rb')),\n",
    "          'lr_bal':pickle.load(open('lr_bal.pickle','rb')),\n",
    "          'lr_unbal':pickle.load(open('lr_unbal.pickle','rb')),\n",
    "          'ann_bal':pickle.load(open('ann_bal.pickle','rb')),\n",
    "          'ann_unbal':pickle.load(open('ann_unbal.pickle','rb'))\n",
    "         }\n",
    "prob_models = {'xgb_bal':pickle.load(open('xgb_bal.pickle','rb')),\n",
    "          'xgb_unbal':pickle.load(open('xgb_unbal.pickle','rb')),\n",
    "          'lr_bal':pickle.load(open('lr_bal.pickle','rb')),\n",
    "          'lr_unbal':pickle.load(open('lr_unbal.pickle','rb')),\n",
    "          'ann_bal':pickle.load(open('ann_bal.pickle','rb')),\n",
    "          'ann_unbal':pickle.load(open('ann_unbal.pickle','rb'))\n",
    "         }\n",
    "\n",
    "# Retrieve Class Predictions\n",
    "def pick_predict(key,m_dict,df):\n",
    "    if 'ann' in key:\n",
    "        return m_dict[key].predict_classes(df)\n",
    "    if 'xgb' in key:\n",
    "        bnl = m_dict[key].best_ntree_limit\n",
    "        lpreds = m_dict[key].predict(df,ntree_limit=bnl)\n",
    "        return lpreds\n",
    "    else:\n",
    "        return m_dict[key].predict(df)\n",
    "\n",
    "train_preds = []\n",
    "val_preds = []\n",
    "test_preds = []\n",
    "for i in models:\n",
    "    train_preds.append(pick_predict(i,models,x_train[keep_vars10]))\n",
    "    val_preds.append(pick_predict(i,models,x_val[keep_vars10]))\n",
    "    test_preds.append(pick_predict(i,models,x_test[keep_vars10]))\n",
    "\n",
    "# Make array of all predictions\n",
    "t_preds_array = np.array(train_preds).transpose()\n",
    "v_preds_array = np.array(val_preds).transpose()\n",
    "test_preds_array = np.array(test_preds).transpose()\n",
    "\n",
    "# Retrieve Prob Predictions\n",
    "def pick_predict_prob(key,m_dict,df):\n",
    "    if 'ann' in key:\n",
    "        return np.array([i[1] for i in m_dict[key].predict(df)])\n",
    "    if 'xgb' in key:\n",
    "        bnl = m_dict[key].best_ntree_limit\n",
    "        lpreds = m_dict[key].predict_proba(df,ntree_limit=bnl)\n",
    "        return np.array([i[1] for i in lpreds])\n",
    "    else:\n",
    "        return np.array([i[1] for i in m_dict[key].predict_proba(df)])\n",
    "\n",
    "train_preds_prob = []\n",
    "val_preds_prob = []\n",
    "test_preds_prob = []\n",
    "for i in prob_models:\n",
    "    train_preds_prob.append(pick_predict_prob(i,\n",
    "                                              prob_models,\n",
    "                                              x_train[keep_vars10]))\n",
    "    val_preds_prob.append(pick_predict_prob(i,\n",
    "                                            prob_models,\n",
    "                                            x_val[keep_vars10]))\n",
    "    test_preds_prob.append(pick_predict_prob(i,\n",
    "                                            prob_models,\n",
    "                                            x_test[keep_vars10]))\n",
    "\n",
    "# Make array of all predictions\n",
    "t_preds_array_prob = np.array(train_preds_prob).transpose()\n",
    "v_preds_array_prob = np.array(val_preds_prob).transpose()\n",
    "test_preds_array_prob = np.array(test_preds_prob).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These ensemble approaches work best when there's some heterogeneity in predictions across models. Let's take a look at how similar these models' predictions are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.16 percent of training predictions were unanimous.\n",
      "Overall, there was 92.56 percent agreement across models fortraining data on average.\n",
      "Among discordant predictions, there was 71.23 percent agreement across models for training data on average.\n",
      "\n",
      "\n",
      "74.04 percent of training predictions were unanimous.\n",
      "Overall, there was 92.51 percent agreement across models fortraining data on average.\n",
      "Among discordant predictions, there was 71.13 percent agreement across models for training data on average.\n"
     ]
    }
   ],
   "source": [
    "unanimous = [True if sum(i)==0 or sum(i)==i.shape[0] else False \\\n",
    "            for i in t_preds_array]\n",
    "m_agree = [i.sum()/i.shape[0] if i.sum()/i.shape[0] >.5 \\\n",
    "                   else 1-i.sum()/i.shape[0] for i in t_preds_array]\n",
    "disc_agree = np.array(m_agree)[[False if i else True for i in unanimous]]\n",
    "\n",
    "print('%.2f percent of training predictions were unanimous.' \\\n",
    "      %(np.mean(unanimous)*100))\n",
    "print('Overall, there was %.2f percent agreement across models for' \\\n",
    "      %(np.mean(m_agree)*100) + \\\n",
    "      'training data on average.')\n",
    "print('Among discordant predictions, there was %.2f percent agreement' \\\n",
    "      %(np.mean(disc_agree)*100) + \n",
    "      ' across models for training data on average.')\n",
    "\n",
    "unanimous_v = [True if sum(i)==0 or sum(i)==i.shape[0] else False \\\n",
    "            for i in v_preds_array]\n",
    "m_agree_v = [i.sum()/i.shape[0] if i.sum()/i.shape[0] >.5 \\\n",
    "                   else 1-i.sum()/i.shape[0] for i in v_preds_array]\n",
    "disc_agree_v = np.array(m_agree_v)[[False if i else True for i in unanimous_v]]\n",
    "\n",
    "print('\\n\\n%.2f percent of training predictions were unanimous.' \\\n",
    "      %(np.mean(unanimous_v)*100))\n",
    "print('Overall, there was %.2f percent agreement across models for' \\\n",
    "      %(np.mean(m_agree_v)*100) + \\\n",
    "      'training data on average.')\n",
    "print('Among discordant predictions, there was %.2f percent agreement' \\\n",
    "      %(np.mean(disc_agree_v)*100) + \n",
    "      ' across models for training data on average.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the models largely agreed with each other and had similar performance, I don't expect to get much of a performance boost (if any), but I'll go ahead and do it anyway just to see what happens. \n",
    "\n",
    "Next I'll add all the predictions to training dataset, then fit another XGBoost model on the combined data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make stacked array predictions\n",
    "t_stack_p = np.concatenate([t_preds_array_prob,\n",
    "                         t_preds_array],axis=1)\n",
    "v_stack_p = np.concatenate([v_preds_array_prob,\n",
    "                         v_preds_array],axis=1)\n",
    "test_stack_p = np.concatenate([test_preds_array_prob,\n",
    "                         test_preds_array],axis=1)\n",
    "\n",
    "\n",
    "# Make stacked array of features and predictions\n",
    "t_stack_fp = np.concatenate([x_train[keep_vars10],\n",
    "                         t_preds_array_prob,\n",
    "                         t_preds_array],axis=1)\n",
    "v_stack_fp = np.concatenate([x_val[keep_vars10],\n",
    "                         v_preds_array_prob,\n",
    "                         v_preds_array],axis=1)\n",
    "test_stack_fp = np.concatenate([x_test[keep_vars10],\n",
    "                         test_preds_array_prob,\n",
    "                         test_preds_array],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Vote\n",
    "\n",
    "First I'll simply take the majority vote of all the models, then look at the classification report. I'm not tuning any parameters here, so there's no real need for the validation set (I'll compare all models using the test set in the next post), but I'll include it for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "[[109322  48110]\n",
      " [ 28884  78929]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.69      0.74    157432\n",
      "         1.0       0.62      0.73      0.67    107813\n",
      "\n",
      "   micro avg       0.71      0.71      0.71    265245\n",
      "   macro avg       0.71      0.71      0.71    265245\n",
      "weighted avg       0.72      0.71      0.71    265245\n",
      "\n",
      "Validation\n",
      "[[27092 12239]\n",
      " [ 7250 19731]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.69      0.74     39331\n",
      "         1.0       0.62      0.73      0.67     26981\n",
      "\n",
      "   micro avg       0.71      0.71      0.71     66312\n",
      "   macro avg       0.70      0.71      0.70     66312\n",
      "weighted avg       0.72      0.71      0.71     66312\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Train')\n",
    "print(confusion_matrix(y_train,t_preds_array.mean(axis=1)>=.5))\n",
    "print(classification_report(y_train,t_preds_array.mean(axis=1)>=.5))\n",
    "\n",
    "print('Validation')\n",
    "print(confusion_matrix(y_val,v_preds_array.mean(axis=1)>=.5))\n",
    "print(classification_report(y_val,v_preds_array.mean(axis=1)>=.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions as features\n",
    "\n",
    "I'll train a simple XGBoost model using predictions as features. Including both class and probability predictions introduces obvious redundancy, but the model will adjust for that automatically anyway. I'll specify scale_pos_weight and use early stopping (1000 trees, stopping after 20 trees with no improvement in val loss). Then I'll print the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "[[104806  52626]\n",
      " [ 24871  82942]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.67      0.73    157432\n",
      "         1.0       0.61      0.77      0.68    107813\n",
      "\n",
      "   micro avg       0.71      0.71      0.71    265245\n",
      "   macro avg       0.71      0.72      0.71    265245\n",
      "weighted avg       0.73      0.71      0.71    265245\n",
      "\n",
      "Validation\n",
      "[[25960 13371]\n",
      " [ 6320 20661]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.66      0.73     39331\n",
      "         1.0       0.61      0.77      0.68     26981\n",
      "\n",
      "   micro avg       0.70      0.70      0.70     66312\n",
      "   macro avg       0.71      0.71      0.70     66312\n",
      "weighted avg       0.72      0.70      0.71     66312\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf = XGBClassifier(max_depth=3,n_estimators=1000,random_state=1234,\n",
    "                  scale_pos_weight=sum(y_train==0)/sum(y_train==1))\n",
    "pf.fit(t_stack_p,\n",
    "              y_train,\n",
    "        eval_metric=['error','logloss'],\n",
    "        eval_set=[(t_stack_p,\n",
    "                   y_train),\n",
    "                  (v_stack_p,\n",
    "                   y_val)],\n",
    "       early_stopping_rounds=20,\n",
    "             verbose=0)\n",
    "\n",
    "print('Train')\n",
    "print(confusion_matrix(y_train,\n",
    "                       pf.predict(t_stack_p,\n",
    "                                 ntree_limit=pf.best_ntree_limit)))\n",
    "print(classification_report(y_train,\n",
    "                            pf.predict(t_stack_p,\n",
    "                                 ntree_limit=pf.best_ntree_limit)))\n",
    "\n",
    "print('Validation')\n",
    "print(confusion_matrix(y_val,\n",
    "                       pf.predict(v_stack_p,\n",
    "                                 ntree_limit=pf.best_ntree_limit)))\n",
    "print(classification_report(y_val,\n",
    "                            pf.predict(v_stack_p,\n",
    "                                 ntree_limit=pf.best_ntree_limit)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data + predictions as features\n",
    "\n",
    "Finally, I'll train another XGBoost model using both the original training data and model predictions as features. As with th previous model, I'll specify scale_pos_weight and use early stopping (1000 trees, stopping after 20 trees with no improvement in val loss). Then I'll print the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "[[104810  52622]\n",
      " [ 24848  82965]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.67      0.73    157432\n",
      "         1.0       0.61      0.77      0.68    107813\n",
      "\n",
      "   micro avg       0.71      0.71      0.71    265245\n",
      "   macro avg       0.71      0.72      0.71    265245\n",
      "weighted avg       0.73      0.71      0.71    265245\n",
      "\n",
      "Validation\n",
      "[[25960 13371]\n",
      " [ 6326 20655]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.66      0.72     39331\n",
      "         1.0       0.61      0.77      0.68     26981\n",
      "\n",
      "   micro avg       0.70      0.70      0.70     66312\n",
      "   macro avg       0.71      0.71      0.70     66312\n",
      "weighted avg       0.72      0.70      0.71     66312\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpf = XGBClassifier(max_depth=3,n_estimators=1000,random_state=1234,\n",
    "                   scale_pos_weight=sum(y_train==0)/sum(y_train==1))\n",
    "fpf.fit(t_stack_fp,\n",
    "              y_train,\n",
    "        eval_metric=['error','logloss'],\n",
    "        eval_set=[(t_stack_fp,\n",
    "                   y_train),\n",
    "                  (v_stack_fp,\n",
    "                   y_val)],\n",
    "       early_stopping_rounds=20,\n",
    "             verbose=0)\n",
    "\n",
    "print('Train')\n",
    "print(confusion_matrix(y_train,\n",
    "                       fpf.predict(t_stack_fp,\n",
    "                                 ntree_limit=fpf.best_ntree_limit)))\n",
    "print(classification_report(y_train,\n",
    "                            fpf.predict(t_stack_fp,\n",
    "                                 ntree_limit=fpf.best_ntree_limit)))\n",
    "\n",
    "print('Validation')\n",
    "print(confusion_matrix(y_val,\n",
    "                       fpf.predict(v_stack_fp,\n",
    "                                 ntree_limit=fpf.best_ntree_limit)))\n",
    "print(classification_report(y_val,\n",
    "                            fpf.predict(v_stack_fp,\n",
    "                                 ntree_limit=fpf.best_ntree_limit)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next and final post of this series, I'll compare the performance of all these models using the test data I set aside at the very beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Test Class Predictions\n",
    "maj_vote = np.array(test_preds_array.mean(axis=1)>=.5).reshape(-1,1)\n",
    "pf_predict = pf.predict(test_stack_p,\n",
    "                        ntree_limit=pf.best_ntree_limit).reshape(-1,1)\n",
    "fpf_predict = fpf.predict(test_stack_fp,\n",
    "                          ntree_limit=fpf.best_ntree_limit).reshape(-1,1)\n",
    "\n",
    "tc_array = np.concatenate([test_preds_array, maj_vote, \n",
    "                           pf_predict, fpf_predict],\n",
    "              axis=1)\n",
    "\n",
    "test_class_pred = dict(zip(['XGB Unbalanced', 'XGB Balanced',\n",
    "                           'SVM Unbalanced', \"SVM Balanced\",\n",
    "                           'Logistic Unbalanced', 'Logistic Balanced',\n",
    "                           'ANN Unbalanced', 'ANN Balanced',\n",
    "                           'Majority Vote', 'Prediction Stack',\n",
    "                           'Features and Prediction Stack'],\n",
    "                          tc_array.transpose()))\n",
    "\n",
    "# Save Test Probability Predictions\n",
    "maj_vote_proba = np.array(test_preds_array_prob.mean(axis=1)).reshape(-1,1)\n",
    "pf_predict_proba = [i[1] for i in \\\n",
    "                    pf.predict_proba(test_stack_p, \n",
    "                                     ntree_limit=pf.best_ntree_limit)]\n",
    "pf_predict_proba = np.array(pf_predict_proba).reshape(-1,1)\n",
    "\n",
    "fpf_predict_proba = [i[1] for i in \\\n",
    "                     fpf.predict_proba(test_stack_fp,\n",
    "                                       ntree_limit=fpf.best_ntree_limit)]\n",
    "fpf_predict_proba = np.array(fpf_predict_proba).reshape(-1,1)\n",
    "\n",
    "tp_array = np.concatenate([test_preds_array_prob, maj_vote_proba,\n",
    "                           pf_predict_proba, fpf_predict_proba],\n",
    "                          axis=1)\n",
    "\n",
    "test_prob_pred = dict(zip(['XGB Unbalanced', 'XGB Balanced',\n",
    "                           'Logistic Unbalanced', 'Logistic Balanced',\n",
    "                           'ANN Unbalanced', 'ANN Balanced',\n",
    "                           'Majority Vote', 'Prediction Stack',\n",
    "                           'Features and Prediction Stack'],\n",
    "                          tp_array.transpose()))\n",
    "\n",
    "# Pickle for next post\n",
    "pickle.dump(test_class_pred, open('test_class_pred.pickle', 'wb'))\n",
    "pickle.dump(test_prob_pred, open('test_prob_pred.pickle', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
