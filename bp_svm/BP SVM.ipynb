{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import midus_varsum as mvs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a>\n",
    "# Introduction\n",
    "\n",
    "After playing around with the <a href=\"http://midus.wisc.edu/\" target=\"_blank\">MIDUS</a> XML file during my <a href=\"https://alexdatasci.com/a-little-bit-of-xml-and-python/\" target=\"_blank\">last post</a>, I ended up falling down a bit of a rabbit hole trying to figure out the best way to wrangle the information related to the variables available in the MIDUS 1 dataset. My initial goal seemed simple enough: to be able to find variable names by a search string, matched to the variable label, scale, or topic. In terms of the variable label, this proved simple enough; however, the XML file (as far as I could tell) did not have information related to scales or topics. That information _was_ in a pdf that comes with that data. Then I foolishly decided to try to parse the pdf text to extract what I wanted. I knew that it would probably be tedious and time consuming, but just couldn't help myself. Well, I was right. It was tedious and time consuming. I didn't get everything I wanted before I decided I couldn't spend any more time on it, but I did come away with some useful tools. I haven't made a post about it (at least not yet), but the python file that has everything I did (which I'll use here as well, see 'midus_varsum') is on GITHUB_ADDLINK. \n",
    "\n",
    "Anyway, the main reason I wanted to be able to get so much information about the individual variables in the first place was to try out this support vector machine project:\n",
    "\n",
    "__The Scenario__\n",
    "\n",
    "Let's say that someone has a ton of questionnaire data that assesses a lot of different aspects of life for middle aged people (e.g. subjective mental/physical health, family obligations, social support, etc.) as well as objective measures of physical health (e.g. blood pressure, disease, etc.). Given all that information, can we create a tool that uses a reasonable subset of all those measures (say 10-20 out of a couple thousand or so) that can do a decent job of discriminating between those at risk for a particular physical disease or condition? Moreover, can we do that using variables that are not themselves direct proxies for that particular disease or condition? That's what I'll attempt to do here, with the binary high/low blood pressure variable as the outcome variable of interest.\n",
    "\n",
    "__The Data__\n",
    "\n",
    "I'll use data from the MIDUS 1 study. The data is publicly available (you have to register and agree to terms of use before downloading) and hosted at <a href=\"https://www.icpsr.umich.edu/icpsrweb/ICPSR/studies/2760/variables\" target=\"_blank\">ICPSR</a>. The reference is listed below:\n",
    "\n",
    "Brim, Orville G., Baltes, Paul B., Bumpass, Larry L., Cleary, Paul D., Featherman, David L., Hazzard, William R., â€¦ Shweder, Richard A. Midlife in the United States (MIDUS 1), 1995-1996. Ann Arbor, MI: Inter-university Consortium for Political and Social Research [distributor], 2017-11-16. https://doi.org/10.3886/ICPSR02760.v12\n",
    "\n",
    "__The Variables__\n",
    "\n",
    "The goal here is to predict high/low BP based on a reasonable number of _individual_ items. This means I don't want to include fully-constructed scales. I'll put my XML/PDF parsing efforts to work to help with that. I also want to make sure to avoid including predictor variables that that are too synonymous with the outcome variable. For example, a variable that indicates whether someone has heart failure or is on a beta-blocker is probably very predictive, but doesn't really match our goals.\n",
    "\n",
    "__Dealing with a large(ish) dataset.__\n",
    "\n",
    "Most of the research I do in my lab doesn't involve massive datasets. They're usually limited to a few hundred observations at most, with anywhere from 20 to ~1000 variables. Although it's not \"big data\" huge, it is big enough (36 MB, 7,000+ observations, ~2,000 variables) that you notice the lag if you try to load it all at once. This isn't a big deal, but I'll still try to figure out what I need first, then load what's required only, instead of loading everything at once. \n",
    "\n",
    "<a id=\"analysis\"></a>\n",
    "# Analysis Plan\n",
    "\n",
    "* Explore variables\n",
    "* Subset/clean data\n",
    "  * Variables of interest\n",
    "  * Deal with missing, etc.\n",
    "  * Make training, validation, test sets\n",
    "* Train classifier\n",
    "  * Tune C parameter (regularization)\n",
    "* Validate number of predictors\n",
    "  * Balance Type I/II errors against # of predictors\n",
    "* Test model\n",
    "\n",
    "__Explore Variables__\n",
    "\n",
    "First, let's figure out which DV to use. I'll use a little function I wrote that searches variable labels to find those related to blood pressure ('pressure' or 'bp'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Description</th>\n",
       "      <th>Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1PA11BC</td>\n",
       "      <td>Health</td>\n",
       "      <td>High blood pressure</td>\n",
       "      <td>What was the diagnosis - HIGH BLOOD PRESSURE?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1PA29CC</td>\n",
       "      <td>Health</td>\n",
       "      <td>High blood pressure</td>\n",
       "      <td>What did the doctor say it was - HIGH BLOOD PR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1PA33</td>\n",
       "      <td>Health</td>\n",
       "      <td>Taking Rx meds for blood pressure</td>\n",
       "      <td>Are you taking any prescription medications fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1PA32S</td>\n",
       "      <td>Health</td>\n",
       "      <td>Blood pressure reading high</td>\n",
       "      <td>What was the exact reading, if you remember - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1PA32D</td>\n",
       "      <td>Health</td>\n",
       "      <td>Blood pressure reading low</td>\n",
       "      <td>What was the exact reading, if you remember - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A1SA9S</td>\n",
       "      <td>Your Health</td>\n",
       "      <td>High blood pressure</td>\n",
       "      <td>In the past twelve months, have you experience...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A1PA34</td>\n",
       "      <td>Health</td>\n",
       "      <td>Other treatments for BP</td>\n",
       "      <td>(Not including prescription medications) Are y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A1PA30</td>\n",
       "      <td>Health</td>\n",
       "      <td># of Months since last BP test</td>\n",
       "      <td>How long has it been since your last blood pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A1PA31</td>\n",
       "      <td>Health</td>\n",
       "      <td>General results of BP test</td>\n",
       "      <td>At that time, was your blood pressure low, abo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Variable        Topic                        Description  \\\n",
       "0  A1PA11BC       Health                High blood pressure   \n",
       "1  A1PA29CC       Health                High blood pressure   \n",
       "2    A1PA33       Health  Taking Rx meds for blood pressure   \n",
       "3   A1PA32S       Health        Blood pressure reading high   \n",
       "4   A1PA32D       Health         Blood pressure reading low   \n",
       "5    A1SA9S  Your Health                High blood pressure   \n",
       "6    A1PA34       Health            Other treatments for BP   \n",
       "7    A1PA30       Health     # of Months since last BP test   \n",
       "8    A1PA31       Health         General results of BP test   \n",
       "\n",
       "                                            Question  \n",
       "0      What was the diagnosis - HIGH BLOOD PRESSURE?  \n",
       "1  What did the doctor say it was - HIGH BLOOD PR...  \n",
       "2  Are you taking any prescription medications fo...  \n",
       "3  What was the exact reading, if you remember - ...  \n",
       "4  What was the exact reading, if you remember - ...  \n",
       "5  In the past twelve months, have you experience...  \n",
       "6  (Not including prescription medications) Are y...  \n",
       "7  How long has it been since your last blood pre...  \n",
       "8  At that time, was your blood pressure low, abo...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mvs.labsearch('pressure').append(mvs.labsearch('bp'),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What was the exact reading, if you remember - DIASTOLIC?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mvs.labsearch('pressure').append(mvs.labsearch('bp'),ignore_index=True)['Question'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few variables adddress high blood pressure. We'll have to take a closer look to see which one is best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1PA11BC</th>\n",
       "      <th>A1PA29CC</th>\n",
       "      <th>A1PA33</th>\n",
       "      <th>A1PA32S</th>\n",
       "      <th>A1PA32D</th>\n",
       "      <th>A1SA9S</th>\n",
       "      <th>A1PA34</th>\n",
       "      <th>A1PA30</th>\n",
       "      <th>A1PA31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INAPP</td>\n",
       "      <td>INAPP</td>\n",
       "      <td>YES</td>\n",
       "      <td>120</td>\n",
       "      <td>70</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>2</td>\n",
       "      <td>SLIGHTLY RAISED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INAPP</td>\n",
       "      <td>INAPP</td>\n",
       "      <td>NO</td>\n",
       "      <td>DONT KNOW</td>\n",
       "      <td>DONT KNOW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>36</td>\n",
       "      <td>ABOUT NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INAPP</td>\n",
       "      <td>INAPP</td>\n",
       "      <td>NO</td>\n",
       "      <td>144</td>\n",
       "      <td>84</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>LESS THAN 1 MONTH</td>\n",
       "      <td>ABOUT NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INAPP</td>\n",
       "      <td>INAPP</td>\n",
       "      <td>YES</td>\n",
       "      <td>DONT KNOW</td>\n",
       "      <td>DONT KNOW</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>1</td>\n",
       "      <td>ABOUT NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INAPP</td>\n",
       "      <td>INAPP</td>\n",
       "      <td>NO</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>LESS THAN 1 MONTH</td>\n",
       "      <td>SLIGHTLY RAISED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  A1PA11BC A1PA29CC A1PA33    A1PA32S    A1PA32D A1SA9S A1PA34  \\\n",
       "0    INAPP    INAPP    YES        120         70     NO     NO   \n",
       "1    INAPP    INAPP     NO  DONT KNOW  DONT KNOW    NaN     NO   \n",
       "2    INAPP    INAPP     NO        144         84     NO     NO   \n",
       "3    INAPP    INAPP    YES  DONT KNOW  DONT KNOW    YES    YES   \n",
       "4    INAPP    INAPP     NO        140         90    YES     NO   \n",
       "\n",
       "              A1PA30           A1PA31  \n",
       "0                  2  SLIGHTLY RAISED  \n",
       "1                 36     ABOUT NORMAL  \n",
       "2  LESS THAN 1 MONTH     ABOUT NORMAL  \n",
       "3                  1     ABOUT NORMAL  \n",
       "4  LESS THAN 1 MONTH  SLIGHTLY RAISED  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bp_vars = mvs.labsearch('pressure').append(mvs.labsearch('bp'),ignore_index=True)['Variable']\n",
    "tpath = '/Users/alex/Documents/alexdatasci/data_files/MIDUS_1/ICPSR_02760' \\\n",
    "    + '/DS0001/02760-0001-Data.dta'\n",
    "bp_df = pd.read_stata(tpath,columns=bp_vars)\n",
    "bp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can already tell that some of these variables aren't gonna work. Let's see which of these variables is complete (or close to it). First, which variables have a lot of 'INAPP' (meaning not answered)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A1PA11BC    6192\n",
       "A1PA29CC    6700\n",
       "A1PA33         0\n",
       "A1PA32S       86\n",
       "A1PA32D       86\n",
       "A1SA9S         0\n",
       "A1PA34         0\n",
       "A1PA30         0\n",
       "A1PA31        11\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bp_df.apply(lambda x: sum(x=='INAPP'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"A1PA32S\" and \"A1PA32D\" represent Systolic and Diastolic BP readings, respectively (self-reported). How many of those do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Known</th>\n",
       "      <th>Unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A1PA32S</th>\n",
       "      <td>3169</td>\n",
       "      <td>4025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A1PA32D</th>\n",
       "      <td>3167</td>\n",
       "      <td>4027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Known  Unknown\n",
       "A1PA32S   3169     4025\n",
       "A1PA32D   3167     4027"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    'Known': bp_df.iloc[:,3:5].apply(lambda x: sum(~(x=='DONT KNOW') | (x=='INAPP'))),\n",
    "    'Unknown': bp_df.iloc[:,3:5].apply(lambda x: sum((x=='DONT KNOW') | (x=='INAPP')))\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks bad, but the data is actually two questionnaires. ~7000 completed the phone portion, while about half that completed an additional \"Self-Administered\" questionniare (SAQ). We'll probably stick to those who completed both anyway, which makes this look pretty good. Finally, what about the last two variables (A1PA30 and A1PA31), which represent time since last reading, and whether that reading was high or low?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many missing time since BP reading?\n",
    "sum((bp_df['A1PA30']=='DONT KNOW') | (bp_df['A1PA30']=='NEVER'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ABOUT NORMAL       5128\n",
       "LOW                 921\n",
       "SLIGHTLY RAISED     819\n",
       "HIGH                154\n",
       "DONT KNOW            74\n",
       "INAPP                11\n",
       "REFUSED/MISSING       1\n",
       "Name: A1PA31, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# value_counts for BP reading (categorical)\n",
    "bp_df['A1PA31'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good. We want the BP reading to be fairly recent, so we can create a variable that indicates SLIGHTLY RAISED or HIGH BP within the last 12 months:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.0    5588\n",
       " 1.0     872\n",
       "NaN      648\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_bp = np.where((bp_df['A1PA30']<=12) & \\\n",
    "                   ((bp_df['A1PA31']=='SLIGHTLY RAISED') | \\\n",
    "                    (bp_df['A1PA31']=='HIGH')),\n",
    "                   1,\n",
    "                   np.where((bp_df['A1PA30']=='DONT KNOW') | \\\n",
    "                           (bp_df['A1PA30']=='NEVER') | \\\n",
    "                            (bp_df['A1PA30']>12),\n",
    "                            None,\n",
    "                            0))\n",
    "pd.Series(high_bp).value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so we have a DV. Now lets decide which predictors to include. Let's take a look at the types of variables available in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Phone</td>\n",
       "      <td>Admin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phone</td>\n",
       "      <td>Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Phone</td>\n",
       "      <td>Education, Occupation and Marital Status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phone</td>\n",
       "      <td>Household Roster and Children</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Phone</td>\n",
       "      <td>Living arrangements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Phone</td>\n",
       "      <td>Childhood background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Phone</td>\n",
       "      <td>Life Satisfaction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SAQ</td>\n",
       "      <td>Your Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SAQ</td>\n",
       "      <td>Health Questions for Women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SAQ</td>\n",
       "      <td>Health Insurance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SAQ</td>\n",
       "      <td>Parents' Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SAQ</td>\n",
       "      <td>Childhood Family Background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SAQ</td>\n",
       "      <td>Personal Beliefs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SAQ</td>\n",
       "      <td>Images of Life Change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SAQ</td>\n",
       "      <td>Psychological Turning Points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SAQ</td>\n",
       "      <td>Work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>SAQ</td>\n",
       "      <td>Finances</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SAQ</td>\n",
       "      <td>Community Involvement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SAQ</td>\n",
       "      <td>Your Neighborhood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SAQ</td>\n",
       "      <td>Social Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>SAQ</td>\n",
       "      <td>Children</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>SAQ</td>\n",
       "      <td>Marriage or Close Relationship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>SAQ</td>\n",
       "      <td>Sexuality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>SAQ</td>\n",
       "      <td>Religion and Spirituality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SAQ</td>\n",
       "      <td>Demographics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SAQ</td>\n",
       "      <td>Discrimination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>SAQ</td>\n",
       "      <td>Life Overall</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Source                                     Topic\n",
       "0   Phone                                     Admin\n",
       "1   Phone                                    Health\n",
       "2   Phone  Education, Occupation and Marital Status\n",
       "3   Phone             Household Roster and Children\n",
       "4   Phone                       Living arrangements\n",
       "5   Phone                      Childhood background\n",
       "6   Phone                         Life Satisfaction\n",
       "7     SAQ                               Your Health\n",
       "8     SAQ                Health Questions for Women\n",
       "9     SAQ                          Health Insurance\n",
       "10    SAQ                           Parents' Health\n",
       "11    SAQ               Childhood Family Background\n",
       "12    SAQ                          Personal Beliefs\n",
       "13    SAQ                     Images of Life Change\n",
       "14    SAQ              Psychological Turning Points\n",
       "15    SAQ                                      Work\n",
       "16    SAQ                                  Finances\n",
       "17    SAQ                     Community Involvement\n",
       "18    SAQ                         Your Neighborhood\n",
       "19    SAQ                           Social Networks\n",
       "20    SAQ                                  Children\n",
       "21    SAQ            Marriage or Close Relationship\n",
       "22    SAQ                                 Sexuality\n",
       "23    SAQ                 Religion and Spirituality\n",
       "24    SAQ                              Demographics\n",
       "25    SAQ                            Discrimination\n",
       "26    SAQ                              Life Overall"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mvs.topicsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhat arbitrarily, I'll include variables related to Life Satisfaction, Personal Beliefs, Work, Finances, Community Involvement, Your Neighborhood, Social Networks, and Life Overall. If we were constantly getting a steady stream of all these variables, then including everything could be justified. But since that's not the case, I'll subset. I'll also throw in age and sex.\n",
    "\n",
    "I want to omit any variable that is a scale constructed from many items. This is where my pdf parsing work can help. I was able to extract all the variables that were constructed scales, so I can filter them out. Let's read in the dataset with only the variables I want, then add the high_bp variable. I'll then inspect the shape of the df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Description</th>\n",
       "      <th>Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1PRSEX</td>\n",
       "      <td>Admin</td>\n",
       "      <td>Gender of respondent</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NPRB_SEX</td>\n",
       "      <td>Admin</td>\n",
       "      <td>Prob of selection - gender</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SEX_CELL</td>\n",
       "      <td>Admin</td>\n",
       "      <td># of selected gender in HH</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NPRB_WHO</td>\n",
       "      <td>Admin</td>\n",
       "      <td>Prob of selection - w/in gender grp</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WHO</td>\n",
       "      <td>Admin</td>\n",
       "      <td>Position within gender grp</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A1PB36A1</td>\n",
       "      <td>Household Roster and Children</td>\n",
       "      <td>Gender child 1</td>\n",
       "      <td>Starting with the oldest, is your first child ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A1PB36A2</td>\n",
       "      <td>Household Roster and Children</td>\n",
       "      <td>Gender child 2</td>\n",
       "      <td>Is your second child a male or female?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A1PB36A3</td>\n",
       "      <td>Household Roster and Children</td>\n",
       "      <td>Gender child 3</td>\n",
       "      <td>Is your third child a male or female?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A1PB36A4</td>\n",
       "      <td>Household Roster and Children</td>\n",
       "      <td>Gender child 4</td>\n",
       "      <td>Is your fourth child a male or female?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A1PB36A5</td>\n",
       "      <td>Household Roster and Children</td>\n",
       "      <td>Gender child 5</td>\n",
       "      <td>Is your fifth child a male or female?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A1PB36A6</td>\n",
       "      <td>Household Roster and Children</td>\n",
       "      <td>Gender child 6</td>\n",
       "      <td>Is your sixth child a male or female?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>A1PB36A7</td>\n",
       "      <td>Household Roster and Children</td>\n",
       "      <td>Gender child 7</td>\n",
       "      <td>Is your seventh child a male or female?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>A1PB36A8</td>\n",
       "      <td>Household Roster and Children</td>\n",
       "      <td>Gender child 8</td>\n",
       "      <td>Is your eighth child a male or female?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>A1PB36A9</td>\n",
       "      <td>Household Roster and Children</td>\n",
       "      <td>Gender child 9</td>\n",
       "      <td>Is your nineth child a male or female?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>A1PB36A10</td>\n",
       "      <td>Household Roster and Children</td>\n",
       "      <td>Gender child 10</td>\n",
       "      <td>Is your tenth child a male or female?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A1PB37A1</td>\n",
       "      <td>Household Roster and Children</td>\n",
       "      <td>Gender NB child 1</td>\n",
       "      <td>Is your oldest non-biological child a male or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A1PB37A2</td>\n",
       "      <td>Household Roster and Children</td>\n",
       "      <td>Gender NB child 2</td>\n",
       "      <td>Is your second non-biological child a male or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A1PB37A3</td>\n",
       "      <td>Household Roster and Children</td>\n",
       "      <td>Gender NB child 3</td>\n",
       "      <td>Is your third non-biological child a male or f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A1PB37A4</td>\n",
       "      <td>Household Roster and Children</td>\n",
       "      <td>Gender NB child 4</td>\n",
       "      <td>Is your fourth non-biological child a male or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>A1PB37A5</td>\n",
       "      <td>Household Roster and Children</td>\n",
       "      <td>Gender NB child 5</td>\n",
       "      <td>Is your fifth non-biological child a male or f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>A1PB37A6</td>\n",
       "      <td>Household Roster and Children</td>\n",
       "      <td>Gender NB child 6</td>\n",
       "      <td>Is your sixth non-biological child a male or f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>A1PB37A7</td>\n",
       "      <td>Household Roster and Children</td>\n",
       "      <td>Gender NB child 7</td>\n",
       "      <td>Is your seventh non-biological child a male or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>A1PB37A8</td>\n",
       "      <td>Household Roster and Children</td>\n",
       "      <td>Gender NB child 8</td>\n",
       "      <td>Is your eighth non-biological child a male or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>A1PB37A9</td>\n",
       "      <td>Household Roster and Children</td>\n",
       "      <td>Gender NB child 9</td>\n",
       "      <td>Is your nineth non-biological child a male or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>A1PB37A10</td>\n",
       "      <td>Household Roster and Children</td>\n",
       "      <td>Gender NB child 10</td>\n",
       "      <td>Is your tenth non-biological child a male or f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>A1SS15_2</td>\n",
       "      <td>Discrimination</td>\n",
       "      <td>Discrim_gender</td>\n",
       "      <td>What was the main reason for the discriminatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Variable                          Topic  \\\n",
       "0     A1PRSEX                          Admin   \n",
       "1    NPRB_SEX                          Admin   \n",
       "2    SEX_CELL                          Admin   \n",
       "3    NPRB_WHO                          Admin   \n",
       "4         WHO                          Admin   \n",
       "5    A1PB36A1  Household Roster and Children   \n",
       "6    A1PB36A2  Household Roster and Children   \n",
       "7    A1PB36A3  Household Roster and Children   \n",
       "8    A1PB36A4  Household Roster and Children   \n",
       "9    A1PB36A5  Household Roster and Children   \n",
       "10   A1PB36A6  Household Roster and Children   \n",
       "11   A1PB36A7  Household Roster and Children   \n",
       "12   A1PB36A8  Household Roster and Children   \n",
       "13   A1PB36A9  Household Roster and Children   \n",
       "14  A1PB36A10  Household Roster and Children   \n",
       "15   A1PB37A1  Household Roster and Children   \n",
       "16   A1PB37A2  Household Roster and Children   \n",
       "17   A1PB37A3  Household Roster and Children   \n",
       "18   A1PB37A4  Household Roster and Children   \n",
       "19   A1PB37A5  Household Roster and Children   \n",
       "20   A1PB37A6  Household Roster and Children   \n",
       "21   A1PB37A7  Household Roster and Children   \n",
       "22   A1PB37A8  Household Roster and Children   \n",
       "23   A1PB37A9  Household Roster and Children   \n",
       "24  A1PB37A10  Household Roster and Children   \n",
       "25   A1SS15_2                 Discrimination   \n",
       "\n",
       "                            Description  \\\n",
       "0                  Gender of respondent   \n",
       "1            Prob of selection - gender   \n",
       "2            # of selected gender in HH   \n",
       "3   Prob of selection - w/in gender grp   \n",
       "4            Position within gender grp   \n",
       "5                        Gender child 1   \n",
       "6                        Gender child 2   \n",
       "7                        Gender child 3   \n",
       "8                        Gender child 4   \n",
       "9                        Gender child 5   \n",
       "10                       Gender child 6   \n",
       "11                       Gender child 7   \n",
       "12                       Gender child 8   \n",
       "13                       Gender child 9   \n",
       "14                      Gender child 10   \n",
       "15                    Gender NB child 1   \n",
       "16                    Gender NB child 2   \n",
       "17                    Gender NB child 3   \n",
       "18                    Gender NB child 4   \n",
       "19                    Gender NB child 5   \n",
       "20                    Gender NB child 6   \n",
       "21                    Gender NB child 7   \n",
       "22                    Gender NB child 8   \n",
       "23                    Gender NB child 9   \n",
       "24                   Gender NB child 10   \n",
       "25                       Discrim_gender   \n",
       "\n",
       "                                             Question  \n",
       "0                                                None  \n",
       "1                                                None  \n",
       "2                                                None  \n",
       "3                                                None  \n",
       "4                                                None  \n",
       "5   Starting with the oldest, is your first child ...  \n",
       "6              Is your second child a male or female?  \n",
       "7               Is your third child a male or female?  \n",
       "8              Is your fourth child a male or female?  \n",
       "9               Is your fifth child a male or female?  \n",
       "10              Is your sixth child a male or female?  \n",
       "11            Is your seventh child a male or female?  \n",
       "12             Is your eighth child a male or female?  \n",
       "13             Is your nineth child a male or female?  \n",
       "14              Is your tenth child a male or female?  \n",
       "15  Is your oldest non-biological child a male or ...  \n",
       "16  Is your second non-biological child a male or ...  \n",
       "17  Is your third non-biological child a male or f...  \n",
       "18  Is your fourth non-biological child a male or ...  \n",
       "19  Is your fifth non-biological child a male or f...  \n",
       "20  Is your sixth non-biological child a male or f...  \n",
       "21  Is your seventh non-biological child a male or...  \n",
       "22  Is your eighth non-biological child a male or ...  \n",
       "23  Is your nineth non-biological child a male or ...  \n",
       "24  Is your tenth non-biological child a male or f...  \n",
       "25  What was the main reason for the discriminatio...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mvs.labsearch('age')\n",
    "mvs.labsearch('gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7108, 475)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the scale variables (want to omit these)\n",
    "scalevars = mvs.scaledf['Varname'][~mvs.scaledf['Varname'].isna()]\n",
    "# Get topic variables\n",
    "topics = ['Life Satisfaction', 'Personal Beliefs', 'Work', 'Finances', \n",
    "          'Community Involvement', 'Your Neighborhood', \n",
    "          'Social Networks', 'Life Overall']\n",
    "# varbytopic is a function I wrote that returns variables by topic by parsing XML\n",
    "topicvars = [mvs.varbytopic(i)['Variable'] for i in topics]\n",
    "topicvars = pd.Series([i for j in topicvars for i in j])\n",
    "# Drop the constructed scale variables\n",
    "topicvars = topicvars[~topicvars.isin(scalevars)]\n",
    "topicvars = topicvars.append(pd.Series(['A1PAGE_M2','A1PRSEX']),\n",
    "                             ignore_index=True)\n",
    "# Import data from the STATA file.\n",
    "df = pd.read_stata(tpath,columns=topicvars)\n",
    "# Add the high_bp DV\n",
    "df['high_bp'] = high_bp\n",
    "# Get shape\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I already know that we're only going to be able to use some of these observations. Let's see how many are complete cases. First, convert \"DONT KNOW\", \"INAPP\", and \"REFUSED/MISSING\" to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_vals = {'DONT KNOW':np.nan, 'REFUSED/MISSING':np.nan, 'INAPP':np.nan}\n",
    "df = df.replace(replace_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll generate a histogram to see frequencies of missing values among the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 13., 383.,  15.,  57.,   3.,   0.,   0.,   0.,   1.,   3.]),\n",
       " array([   0. ,  710.8, 1421.6, 2132.4, 2843.2, 3554. , 4264.8, 4975.6,\n",
       "        5686.4, 6397.2, 7108. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "varmiss = df.apply(lambda x: sum(x.isna()))\n",
    "plt.hist(varmiss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most variables have about 1000 missing values. I'll plot the sample size I can get by the number of variables (sorted by number of missing values) kept in the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258    2605.0\n",
       "259    2586.0\n",
       "260    2549.0\n",
       "261    2534.0\n",
       "262    2516.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df[varmiss.sort_values().index]\n",
    "sample_size = np.array([np.nan]).repeat(df.shape[1])\n",
    "cum_cc = np.array([False]).repeat(df.shape[0])\n",
    "for i in range(df.shape[1]):\n",
    "    cum_cc = pd.DataFrame({'prev_index':cum_cc,\n",
    "                           'current_index':df.iloc[:,i].isna()}).sum(axis=1)!=0\n",
    "    sample_size[i] = sum(cum_cc==False)\n",
    "plt.plot(sample_size)\n",
    "plt.axhline(2500)\n",
    "pd.Series(sample_size[sample_size>=2500]).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that I want at least 500 cases in my test set, and I want my test set to be 20% of the total set. That means I'll need a total sample size of at least 2500, which means I can include 262 of my variables of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:,:262]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these variables are ordinal, some are categorical, and some appear categorical but are actually continuous. More cleaning/recoding is required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_list = list(np.array([np.nan]).repeat(sum(df.dtypes=='object')))\n",
    "for i in range(len(vc_list)):\n",
    "    valcounts = df.loc[:,pd.Series(df.dtypes=='object')].iloc[:,i].value_counts()\n",
    "    onlystrs = [i for i in valcounts.index if type(i)==str]\n",
    "    ordstrings = pd.Series(onlystrs).sort_values().reset_index(drop=True)\n",
    "    vc_list[i] = pd.DataFrame({i:ordstrings}).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "obvars = pd.concat(vc_list,ignore_index=True)\n",
    "obvars.index = df.loc[:,pd.Series(df.dtypes=='object')].columns\n",
    "obkeys = obvars.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['A1PAGE_M2'].replace('REFUSED',np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['A1PAGE_M2'] = df['A1PAGE_M2'].astype('float',copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['female'] = [1 if i.lower()=='female' \\\n",
    "                else 0 if i.lower()=='male' \\\n",
    "                else np.nan \\\n",
    "                for i in df['A1PRSEX']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('A1PRSEX',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "obkeys_type = ['ord', 'ord','dummy','cat/ord','ord',\n",
    "               'cont','cont','cont','cat/ord','ord',\n",
    "               'cat/ord','ord','cont','ord','cat/ord',\n",
    "               'cat/ord','ord','cat','cat','cont','dummy']\n",
    "obkeylist = [\n",
    "    dict(zip(obkeys.transpose()[0][~obkeys.transpose()[0].isna()],[1,3,0,2])),\n",
    "    dict(zip(obkeys.transpose()[1][~obkeys.transpose()[1].isna()],[4,1,2,0,3])),\n",
    "    dict(zip(obkeys.transpose()[2][~obkeys.transpose()[2].isna()],[0,1])),\n",
    "    dict(zip(obkeys.transpose()[3][~obkeys.transpose()[3].isna()],[0,1,2])),\n",
    "    dict(zip(obkeys.transpose()[4][~obkeys.transpose()[4].isna()],[1,3,0,2])),\n",
    "    dict(zip(obkeys.transpose()[5][~obkeys.transpose()[5].isna()],[0,10])),\n",
    "    dict(zip(obkeys.transpose()[6][~obkeys.transpose()[6].isna()],[10,0])),\n",
    "    dict(zip(obkeys.transpose()[7][~obkeys.transpose()[7].isna()],[0,10])),\n",
    "    dict(zip(obkeys.transpose()[8][~obkeys.transpose()[8].isna()],[1,2,0])),\n",
    "    dict(zip(obkeys.transpose()[9][~obkeys.transpose()[9].isna()],[0,1,2,3])),\n",
    "    dict(zip(obkeys.transpose()[10][~obkeys.transpose()[10].isna()],[2,3,5,1,0,4])),\n",
    "    dict(zip(obkeys.transpose()[11][~obkeys.transpose()[11].isna()],[0,3,1,2])),\n",
    "    dict(zip(obkeys.transpose()[12][~obkeys.transpose()[12].isna()],[0,10])),\n",
    "    dict(zip(obkeys.transpose()[13][~obkeys.transpose()[13].isna()],[1,2,3,-1,-2,-3])),\n",
    "    dict(zip(obkeys.transpose()[14][~obkeys.transpose()[14].isna()],[3,6,2,4,1,0,7,5])),\n",
    "    dict(zip(obkeys.transpose()[15][~obkeys.transpose()[15].isna()],[4,1,5,0,2,3])),\n",
    "    dict(zip(obkeys.transpose()[16][~obkeys.transpose()[16].isna()],[4,3,0,1,2])),\n",
    "    dict(zip(obkeys.transpose()[17][~obkeys.transpose()[17].isna()],\n",
    "        obkeys.transpose()[17][~obkeys.transpose()[17].isna()])),\n",
    "    dict(zip(obkeys.transpose()[18][~obkeys.transpose()[18].isna()],\n",
    "        obkeys.transpose()[18][~obkeys.transpose()[18].isna()])),\n",
    "    dict(zip(obkeys.transpose()[19][~obkeys.transpose()[19].isna()],[0])),\n",
    "    dict(zip(obkeys.transpose()[20][~obkeys.transpose()[20].isna()],[0,1]))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(obvars.shape[0]):\n",
    "    for j in range(len(obkeylist)):\n",
    "        if obkeys.iloc[j].isin(obvars.iloc[i]).all():\n",
    "            df[obvars.iloc[i].name] = df[obvars.iloc[i].name].replace(obkeylist[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take look at the remaining 'category' variables to make sure they're coded correctly. If they are actually continuous, recode to make them float64. If they are actually categorical, make dummy variables. First I'll check how many of the 'category' variables have numeric (i.e. int or float) observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A1SJ11M      5907\n",
       "A1SJ12          0\n",
       "A1SJ8M       5816\n",
       "A1SCVOB3     6254\n",
       "A1SJ9M       5817\n",
       "A1SHWEARN    6061\n",
       "A1SJ10M      5737\n",
       "A1SMAR       6279\n",
       "A1SHHTOT     6047\n",
       "A1SJ13M      5813\n",
       "A1SJ13          0\n",
       "SKIP_SI7        0\n",
       "A1SJ12M      5974\n",
       "A1SASSET     5674\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catvars = df.dtypes[df.dtypes=='category'].index\n",
    "df[catvars].apply(lambda x: sum([True if (type(i)==int or type(i)==float) and \\\n",
    "                                 np.isnan(i)==False else False for i in x]),\n",
    "                  axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll put that in some context, and take a look at the value counts (non-numeric)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$25,000 OR MORE</td>\n",
       "      <td>NOT CALCULATED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$0/NONE</td>\n",
       "      <td>$1-1000</td>\n",
       "      <td>$1000-1999</td>\n",
       "      <td>$10000-10999</td>\n",
       "      <td>$11000-11999</td>\n",
       "      <td>$12000-12999</td>\n",
       "      <td>$13000-13999</td>\n",
       "      <td>$14000-14999</td>\n",
       "      <td>$15000-15999</td>\n",
       "      <td>$16000-16999</td>\n",
       "      <td>...</td>\n",
       "      <td>$9000-9999</td>\n",
       "      <td>LESS THAN $0/LOSS</td>\n",
       "      <td>MISSING DATA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$100,000 OR MORE</td>\n",
       "      <td>NOT CALCULATED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NOT CALCULATED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$100,000 OR MORE</td>\n",
       "      <td>NOT CALCULATED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Not Calculated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>$100,000 OR MORE</td>\n",
       "      <td>NOT CALCULATED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NOT CALCULATED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>$300,000 OR MORE</td>\n",
       "      <td>NOT CALCULATED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>$100,000 OR MORE</td>\n",
       "      <td>NOT CALCULATED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>$0/NONE</td>\n",
       "      <td>$1-1000</td>\n",
       "      <td>$1000-1999</td>\n",
       "      <td>$10000-10999</td>\n",
       "      <td>$100000 OR MORE</td>\n",
       "      <td>$11000-11999</td>\n",
       "      <td>$12000-12999</td>\n",
       "      <td>$13000-13999</td>\n",
       "      <td>$14000-14999</td>\n",
       "      <td>$15000-15999</td>\n",
       "      <td>...</td>\n",
       "      <td>$45000-49999</td>\n",
       "      <td>$5000-5999</td>\n",
       "      <td>$50000-74999</td>\n",
       "      <td>$6000-6999</td>\n",
       "      <td>$7000-7999</td>\n",
       "      <td>$75000-99999</td>\n",
       "      <td>$8000-8999</td>\n",
       "      <td>$9000-9999</td>\n",
       "      <td>LESS THAN $0/LOSS</td>\n",
       "      <td>MISSING DATA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NO SKIP</td>\n",
       "      <td>SKIP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>$25,000 OR MORE</td>\n",
       "      <td>NOT CALCULATED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NOT CALCULATED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0               1           2             3   \\\n",
       "0    $25,000 OR MORE  NOT CALCULATED         NaN           NaN   \n",
       "1            $0/NONE         $1-1000  $1000-1999  $10000-10999   \n",
       "2   $100,000 OR MORE  NOT CALCULATED         NaN           NaN   \n",
       "3     NOT CALCULATED             NaN         NaN           NaN   \n",
       "4   $100,000 OR MORE  NOT CALCULATED         NaN           NaN   \n",
       "5     Not Calculated             NaN         NaN           NaN   \n",
       "6   $100,000 OR MORE  NOT CALCULATED         NaN           NaN   \n",
       "7     NOT CALCULATED             NaN         NaN           NaN   \n",
       "8   $300,000 OR MORE  NOT CALCULATED         NaN           NaN   \n",
       "9   $100,000 OR MORE  NOT CALCULATED         NaN           NaN   \n",
       "10           $0/NONE         $1-1000  $1000-1999  $10000-10999   \n",
       "11           NO SKIP            SKIP         NaN           NaN   \n",
       "12   $25,000 OR MORE  NOT CALCULATED         NaN           NaN   \n",
       "13    NOT CALCULATED             NaN         NaN           NaN   \n",
       "\n",
       "                 4             5             6             7             8   \\\n",
       "0               NaN           NaN           NaN           NaN           NaN   \n",
       "1      $11000-11999  $12000-12999  $13000-13999  $14000-14999  $15000-15999   \n",
       "2               NaN           NaN           NaN           NaN           NaN   \n",
       "3               NaN           NaN           NaN           NaN           NaN   \n",
       "4               NaN           NaN           NaN           NaN           NaN   \n",
       "5               NaN           NaN           NaN           NaN           NaN   \n",
       "6               NaN           NaN           NaN           NaN           NaN   \n",
       "7               NaN           NaN           NaN           NaN           NaN   \n",
       "8               NaN           NaN           NaN           NaN           NaN   \n",
       "9               NaN           NaN           NaN           NaN           NaN   \n",
       "10  $100000 OR MORE  $11000-11999  $12000-12999  $13000-13999  $14000-14999   \n",
       "11              NaN           NaN           NaN           NaN           NaN   \n",
       "12              NaN           NaN           NaN           NaN           NaN   \n",
       "13              NaN           NaN           NaN           NaN           NaN   \n",
       "\n",
       "              9   ...            22                 23            24  \\\n",
       "0            NaN  ...           NaN                NaN           NaN   \n",
       "1   $16000-16999  ...    $9000-9999  LESS THAN $0/LOSS  MISSING DATA   \n",
       "2            NaN  ...           NaN                NaN           NaN   \n",
       "3            NaN  ...           NaN                NaN           NaN   \n",
       "4            NaN  ...           NaN                NaN           NaN   \n",
       "5            NaN  ...           NaN                NaN           NaN   \n",
       "6            NaN  ...           NaN                NaN           NaN   \n",
       "7            NaN  ...           NaN                NaN           NaN   \n",
       "8            NaN  ...           NaN                NaN           NaN   \n",
       "9            NaN  ...           NaN                NaN           NaN   \n",
       "10  $15000-15999  ...  $45000-49999         $5000-5999  $50000-74999   \n",
       "11           NaN  ...           NaN                NaN           NaN   \n",
       "12           NaN  ...           NaN                NaN           NaN   \n",
       "13           NaN  ...           NaN                NaN           NaN   \n",
       "\n",
       "            25          26            27          28          29  \\\n",
       "0          NaN         NaN           NaN         NaN         NaN   \n",
       "1          NaN         NaN           NaN         NaN         NaN   \n",
       "2          NaN         NaN           NaN         NaN         NaN   \n",
       "3          NaN         NaN           NaN         NaN         NaN   \n",
       "4          NaN         NaN           NaN         NaN         NaN   \n",
       "5          NaN         NaN           NaN         NaN         NaN   \n",
       "6          NaN         NaN           NaN         NaN         NaN   \n",
       "7          NaN         NaN           NaN         NaN         NaN   \n",
       "8          NaN         NaN           NaN         NaN         NaN   \n",
       "9          NaN         NaN           NaN         NaN         NaN   \n",
       "10  $6000-6999  $7000-7999  $75000-99999  $8000-8999  $9000-9999   \n",
       "11         NaN         NaN           NaN         NaN         NaN   \n",
       "12         NaN         NaN           NaN         NaN         NaN   \n",
       "13         NaN         NaN           NaN         NaN         NaN   \n",
       "\n",
       "                   30            31  \n",
       "0                 NaN           NaN  \n",
       "1                 NaN           NaN  \n",
       "2                 NaN           NaN  \n",
       "3                 NaN           NaN  \n",
       "4                 NaN           NaN  \n",
       "5                 NaN           NaN  \n",
       "6                 NaN           NaN  \n",
       "7                 NaN           NaN  \n",
       "8                 NaN           NaN  \n",
       "9                 NaN           NaN  \n",
       "10  LESS THAN $0/LOSS  MISSING DATA  \n",
       "11                NaN           NaN  \n",
       "12                NaN           NaN  \n",
       "13                NaN           NaN  \n",
       "\n",
       "[14 rows x 32 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc_list = list(np.array([np.nan]).repeat(sum(df.dtypes=='category')))\n",
    "for i in range(len(vc_list)):\n",
    "    valcounts = df.loc[:,pd.Series(df.dtypes=='category')].iloc[:,i].value_counts()\n",
    "    onlystrs = [i for i in valcounts.index if type(i)==str]\n",
    "    ordstrings = pd.Series(onlystrs).sort_values().reset_index(drop=True)\n",
    "    vc_list[i] = pd.DataFrame({i:ordstrings}).transpose()\n",
    "catvals = pd.concat(vc_list,ignore_index=True)\n",
    "catvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'SKIP_SI7' variable doesn't actually give us information. I'll drop it. The other two variables that don't have any numbers look like they are money related. Ordinal money-related variables are really annoying to deal with, because they often are not spaced in a way that is conducive to translating to numeric values. They also tend to have a very wide range of values. In this case, since they have quite a few missing values hidden as 'MISSING DATA' (over 1000 and 300), I'll just drop those as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A1SJ11M      category\n",
       "A1SJ12       category\n",
       "A1SJ8M       category\n",
       "A1SCVOB3     category\n",
       "A1SJ9M       category\n",
       "A1SHWEARN    category\n",
       "A1SJ10M      category\n",
       "A1SMAR       category\n",
       "A1SHHTOT     category\n",
       "A1SJ13M      category\n",
       "A1SJ13       category\n",
       "SKIP_SI7     category\n",
       "A1SJ12M      category\n",
       "A1SASSET     category\n",
       "dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[catvars].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['A1SJ12', 'A1SJ13', 'SKIP_SI7'], dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catvars[[1,10,11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(catvars[[1,10,11]],axis=1)\n",
    "catvars = catvars.drop(catvars[[1,10,11]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the 'category' variables all have \"not calculated\" included among otherwise-numeric data. I'll interpret 'not calculated' as missing. Depending on how many are missing, I might just drop them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A1SJ11M      360\n",
       "A1SJ8M       343\n",
       "A1SCVOB3      71\n",
       "A1SJ9M       397\n",
       "A1SHWEARN    264\n",
       "A1SJ10M      404\n",
       "A1SMAR        46\n",
       "A1SHHTOT     215\n",
       "A1SJ13M      392\n",
       "A1SJ12M      332\n",
       "A1SASSET     651\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[catvars].apply(lambda x: sum([str(i).lower()=='not calculated' \\\n",
    "                                for i in x]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['A1SJ11M', 'A1SJ8M', 'A1SCVOB3', 'A1SJ9M', 'A1SHWEARN', 'A1SJ10M',\n",
       "       'A1SMAR', 'A1SHHTOT', 'A1SJ13M', 'A1SJ12M', 'A1SASSET'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catvars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know from earlier that if I lose more than 16 observations, I won't keep 500 in my test set. There are too many missing observations here, so I'll drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(catvars,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I'll make dummy variables for the remaining categorical (i.e. 'object') variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1PAGE_M2</th>\n",
       "      <th>A1PD4</th>\n",
       "      <th>A1PD7</th>\n",
       "      <th>A1PD8</th>\n",
       "      <th>A1PD6</th>\n",
       "      <th>A1PD5</th>\n",
       "      <th>A1PD3</th>\n",
       "      <th>A1PD1</th>\n",
       "      <th>A1PD2</th>\n",
       "      <th>A1PD9</th>\n",
       "      <th>...</th>\n",
       "      <th>A1SK9E</th>\n",
       "      <th>A1SK12E</th>\n",
       "      <th>A1SF1O</th>\n",
       "      <th>A1SI2</th>\n",
       "      <th>A1SK11E</th>\n",
       "      <th>A1SK13C</th>\n",
       "      <th>A1SK8D</th>\n",
       "      <th>A1SF1Y</th>\n",
       "      <th>A1SF1BB</th>\n",
       "      <th>female</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 248 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   A1PAGE_M2  A1PD4  A1PD7  A1PD8  A1PD6  A1PD5  A1PD3  A1PD1  A1PD2  A1PD9  \\\n",
       "0       53.0    3.0    2.0    3.0    2.0    2.0    3.0    3.0    3.0    0.0   \n",
       "1       60.0    2.0    2.0    3.0    2.0    1.0    3.0    3.0    2.0    1.0   \n",
       "2       69.0    3.0    3.0    4.0    3.0    0.0    3.0    3.0    3.0    1.0   \n",
       "3       70.0    3.0    3.0    4.0    3.0    0.0    3.0    3.0    3.0    1.0   \n",
       "4       51.0    2.0    3.0    3.0    3.0    3.0    3.0    3.0    3.0    1.0   \n",
       "\n",
       "   ...  A1SK9E A1SK12E A1SF1O  A1SI2 A1SK11E  A1SK13C  A1SK8D A1SF1Y A1SF1BB  \\\n",
       "0  ...       0     0.0    6.0    8.0     0.0      0.0       0    4.0     4.0   \n",
       "1  ...     NaN     NaN    NaN    NaN     NaN      NaN     NaN    NaN     NaN   \n",
       "2  ...       2     0.0    3.0    8.0     2.0      0.0       0    1.0     1.0   \n",
       "3  ...       0     5.0    1.0    0.0     1.0      5.0     NaN    0.0     0.0   \n",
       "4  ...       0     0.0    4.0    2.0     0.0      0.0       0    6.0     3.0   \n",
       "\n",
       "  female  \n",
       "0    0.0  \n",
       "1    0.0  \n",
       "2    0.0  \n",
       "3    1.0  \n",
       "4    1.0  \n",
       "\n",
       "[5 rows x 248 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make dummies for the remaining categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(df[df.dtypes[df.dtypes!='float64'].index],\n",
    "              drop_first=True,\n",
    "              prefix=df.dtypes[df.dtypes!='float64'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X, df[df.dtypes[df.dtypes=='float64'].index]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "high_bp_1             uint8\n",
       "A1SF3FF_10            uint8\n",
       "A1SF3FF_NOT AT ALL    uint8\n",
       "A1SF3FF_SOME          uint8\n",
       "A1SF3KK_10            uint8\n",
       "A1SF3KK_NOT AT ALL    uint8\n",
       "A1SF3KK_SOME          uint8\n",
       "A1SF4F_10             uint8\n",
       "A1SF4F_NOT AT ALL     uint8\n",
       "A1SF4F_SOME           uint8\n",
       "A1SF3JJ_10            uint8\n",
       "A1SF3JJ_NOT AT ALL    uint8\n",
       "A1SF3JJ_SOME          uint8\n",
       "A1SF3II_10            uint8\n",
       "A1SF3II_NOT AT ALL    uint8\n",
       "A1SF3II_SOME          uint8\n",
       "A1SF3T_10             uint8\n",
       "A1SF3T_NOT AT ALL     uint8\n",
       "A1SF3T_SOME           uint8\n",
       "A1SF3S_10             uint8\n",
       "A1SF3S_NOT AT ALL     uint8\n",
       "A1SF3S_SOME           uint8\n",
       "A1SF3A_10             uint8\n",
       "A1SF3A_NOT AT ALL     uint8\n",
       "A1SF3A_SOME           uint8\n",
       "A1SF3B_10             uint8\n",
       "A1SF3B_NOT AT ALL     uint8\n",
       "A1SF3B_SOME           uint8\n",
       "A1SF3AA_10            uint8\n",
       "A1SF3AA_NOT AT ALL    uint8\n",
       "                      ...  \n",
       "A1SK8D_8.0            uint8\n",
       "A1SK8D_9.0            uint8\n",
       "A1SK8D_10.0           uint8\n",
       "A1SK8D_12.0           uint8\n",
       "A1SK8D_13.0           uint8\n",
       "A1SK8D_14.0           uint8\n",
       "A1SK8D_15.0           uint8\n",
       "A1SK8D_16.0           uint8\n",
       "A1SK8D_17.0           uint8\n",
       "A1SK8D_18.0           uint8\n",
       "A1SK8D_20.0           uint8\n",
       "A1SK8D_22.0           uint8\n",
       "A1SK8D_24.0           uint8\n",
       "A1SK8D_25.0           uint8\n",
       "A1SK8D_27.0           uint8\n",
       "A1SK8D_28.0           uint8\n",
       "A1SK8D_30.0           uint8\n",
       "A1SK8D_32.0           uint8\n",
       "A1SK8D_35.0           uint8\n",
       "A1SK8D_36.0           uint8\n",
       "A1SK8D_40.0           uint8\n",
       "A1SK8D_45.0           uint8\n",
       "A1SK8D_50.0           uint8\n",
       "A1SK8D_60.0           uint8\n",
       "A1SK8D_65.0           uint8\n",
       "A1SK8D_70.0           uint8\n",
       "A1SK8D_72.0           uint8\n",
       "A1SK8D_74.0           uint8\n",
       "A1SK8D_80.0           uint8\n",
       "A1SK8D_150.0          uint8\n",
       "Length: 601, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes[X.dtypes!='float64']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = X.apply(lambda x: len(x.value_counts())==2,axis=0)\n",
    "X.loc[:,~dummies]=preprocessing.scale(X.loc[:,~dummies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['high_bp_1', 'A1SF3FF_10', 'A1SF3FF_NOT AT ALL', 'A1SF3FF_SOME',\n",
       "       'A1SF3KK_10', 'A1SF3KK_NOT AT ALL', 'A1SF3KK_SOME', 'A1SF4F_10',\n",
       "       'A1SF4F_NOT AT ALL', 'A1SF4F_SOME',\n",
       "       ...\n",
       "       'A1SK15G', 'A1SK12B', 'A1SK12E', 'A1SF1O', 'A1SI2', 'A1SK11E',\n",
       "       'A1SK13C', 'A1SF1Y', 'A1SF1BB', 'female'],\n",
       "      dtype='object', length=715)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2027, 507]\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X.drop('high_bp_1',axis=1), \n",
    "                                                    X['high_bp_1'], \n",
    "                                                    test_size = 0.20)\n",
    "print([len(x_train),len(x_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Train SVM\n",
    "\n",
    "Now I'll train the SVM model. I'll do this with a linear kernel first, so that I can get a sense of which variables are actually important (this is not possible if you use a Gaussian kernel). I'll also experiment with different C parameter values (this determines the amount of regularization) to avoid over- or under-fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pos_recall(y_true, y_pred):\n",
    "    ty1 = [True if i==1 else False for i in y_true]\n",
    "    out = sum(np.equal(np.array(y_pred)[ty1],np.array(y_true)[ty1]))/sum(ty1)\n",
    "    return out\n",
    "\n",
    "def pos_f1(y_true, y_pred):\n",
    "    ty1 = [True if i==1 else False for i in y_true]\n",
    "    tp1 = [True if i==1 else False for i in y_pred]\n",
    "    rec = sum(np.equal(np.array(y_pred)[ty1],np.array(y_true)[ty1]))/sum(ty1)\n",
    "    if sum(tp1)==0:\n",
    "        prec=0\n",
    "    else:\n",
    "        prec = sum(np.equal(np.array(y_true)[tp1],np.array(y_pred)[tp1]))/sum(tp1)\n",
    "    if rec+prec == 0:\n",
    "        out = 0\n",
    "    else:\n",
    "        out = 2*((rec*prec)/(rec+prec))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gamma': 0.001, 'C': 100}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10,100]\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "svm_sig = RandomizedSearchCV(svm.SVC(kernel='sigmoid'), param_grid, \n",
    "                           cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=5),\n",
    "                              scoring=make_scorer(pos_f1))\n",
    "svm_sig.fit(x_train.iloc[:,:] ,y_train[:])\n",
    "svm_sig.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mc = {}\n",
    "mc['svm_sig']=pd.Series(\n",
    "    confusion_matrix(y_test, svm_sig.predict(x_test)).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1312   25]\n",
      " [ 101   82]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95      1337\n",
      "           1       0.77      0.45      0.57       183\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      1520\n",
      "   macro avg       0.85      0.71      0.76      1520\n",
      "weighted avg       0.91      0.92      0.91      1520\n",
      "\n",
      "[[790  72]\n",
      " [133  19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.92      0.89       862\n",
      "           1       0.21      0.12      0.16       152\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      1014\n",
      "   macro avg       0.53      0.52      0.52      1014\n",
      "weighted avg       0.76      0.80      0.78      1014\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_train, svm_sig.predict(x_train)))  \n",
    "print(classification_report(y_train, svm_sig.predict(x_train))) \n",
    "\n",
    "print(confusion_matrix(y_test, svm_sig.predict(x_test)))  \n",
    "print(classification_report(y_test, svm_sig.predict(x_test))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gamma': 0.001, 'C': 10}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cs = [0.001, 0.01, 0.1, 1, 10,100]\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "svm_gaus = RandomizedSearchCV(svm.SVC(kernel='rbf'), param_grid, \n",
    "                           cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=5),\n",
    "                              scoring=make_scorer(pos_f1))\n",
    "svm_gaus.fit(x_train.iloc[:,:] ,y_train[:])\n",
    "svm_gaus.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#svm_gaus = svm.SVC(kernel='rbf',C=100,gamma=.001)\n",
    "#svm_gaus.fit(x_train,y_train)\n",
    "mc['svm_gaus']=pd.Series(\n",
    "    confusion_matrix(y_test, svm_gaus.predict(x_test)).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1337    0]\n",
      " [ 136   47]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      1337\n",
      "           1       1.00      0.26      0.41       183\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      1520\n",
      "   macro avg       0.95      0.63      0.68      1520\n",
      "weighted avg       0.92      0.91      0.89      1520\n",
      "\n",
      "[[860   2]\n",
      " [152   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92       862\n",
      "           1       0.00      0.00      0.00       152\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      1014\n",
      "   macro avg       0.42      0.50      0.46      1014\n",
      "weighted avg       0.72      0.85      0.78      1014\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_train, svm_gaus.predict(x_train)))  \n",
    "print(classification_report(y_train, svm_gaus.predict(x_train))) \n",
    "\n",
    "print(confusion_matrix(y_test, svm_gaus.predict(x_test)))  \n",
    "print(classification_report(y_test, svm_gaus.predict(x_test))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:271: UserWarning: The total space of parameters 6 is smaller than n_iter=10. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "/Users/alex/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/alex/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/alex/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/alex/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/alex/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/alex/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/alex/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/alex/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/alex/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/alex/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/alex/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/alex/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/alex/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/alex/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/alex/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/alex/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=<sklearn.model_selection._split.RepeatedStratifiedKFold object at 0x1a25dfa470>,\n",
       "          error_score='raise-deprecating',\n",
       "          estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=4000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "          fit_params=None, iid='warn', n_iter=10, n_jobs=None,\n",
       "          param_distributions={'C': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=make_scorer(pos_f1),\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cs = [0.001, 0.01, 0.1, 1, 10,100]\n",
    "param_grid = {'C': Cs}\n",
    "svm_nokern = RandomizedSearchCV(LinearSVC(max_iter=4000),param_grid,\n",
    "                         cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=5),\n",
    "                              scoring=make_scorer(pos_f1))\n",
    "svm_nokern.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1328    9]\n",
      " [   0  183]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      1337\n",
      "           1       0.95      1.00      0.98       183\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      1520\n",
      "   macro avg       0.98      1.00      0.99      1520\n",
      "weighted avg       0.99      0.99      0.99      1520\n",
      "\n",
      "[[710 152]\n",
      " [108  44]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.82      0.85       862\n",
      "           1       0.22      0.29      0.25       152\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      1014\n",
      "   macro avg       0.55      0.56      0.55      1014\n",
      "weighted avg       0.77      0.74      0.76      1014\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mc['svm_nokern']=pd.Series(\n",
    "    confusion_matrix(y_test, svm_nokern.predict(x_test)).ravel())\n",
    "print(confusion_matrix(y_train, svm_nokern.predict(x_train)))  \n",
    "print(classification_report(y_train, svm_nokern.predict(x_train))) \n",
    "\n",
    "print(confusion_matrix(y_test, svm_nokern.predict(x_test)))  \n",
    "print(classification_report(y_test, svm_nokern.predict(x_test))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare to Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionCV(scoring=make_scorer(pos_recall),max_iter=1000,Cs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/alex/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=20, class_weight=None, cv='warn', dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=1000,\n",
       "           multi_class='warn', n_jobs=None, penalty='l2',\n",
       "           random_state=None, refit=True, scoring=make_scorer(pos_recall),\n",
       "           solver='lbfgs', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1438.44988829])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.C_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1337    0]\n",
      " [   0  183]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1337\n",
      "           1       1.00      1.00      1.00       183\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      1520\n",
      "   macro avg       1.00      1.00      1.00      1520\n",
      "weighted avg       1.00      1.00      1.00      1520\n",
      "\n",
      "[[718 144]\n",
      " [112  40]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85       862\n",
      "           1       0.22      0.26      0.24       152\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      1014\n",
      "   macro avg       0.54      0.55      0.54      1014\n",
      "weighted avg       0.77      0.75      0.76      1014\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mc['lr']=pd.Series(\n",
    "    confusion_matrix(y_test, lr.predict(x_test)).ravel())\n",
    "print(confusion_matrix(y_train, lr.predict(x_train)))  \n",
    "print(classification_report(y_train, lr.predict(x_train))) \n",
    "\n",
    "print(confusion_matrix(y_test, lr.predict(x_test)))  \n",
    "print(classification_report(y_test, lr.predict(x_test))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "combs = pd.DataFrame([(d,w,g,n) \\\n",
    " for d in range(3,9) \\\n",
    "for w in [1,2,5,10,20] \\\n",
    "for g in [.01,.1,1,10,100] \\\n",
    "for n in [10,50,100,1000]])\n",
    "combs.columns = ['max_depth', 'min_child_weight',\n",
    "                'gamma', 'n_estimators']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=<sklearn.model_selection._split.RepeatedStratifiedKFold object at 0x1a23a38d30>,\n",
       "          error_score='raise-deprecating',\n",
       "          estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "          fit_params=None, iid='warn', n_iter=10, n_jobs=None,\n",
       "          param_distributions={'max_depth': [3, 4, 5, 6, 7, 8], 'min_child_weight': [1, 2, 5, 10, 20], 'gamma': [0.01, 0.1, 1, 10, 100], 'n_estimators': [10, 50, 100, 1000]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=make_scorer(pos_f1),\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'max_depth':list(range(3,9)),\n",
    "         'min_child_weight':[1,2,5,10,20],\n",
    "         'gamma':[.01,.1,1,10,100],\n",
    "         'n_estimators':[10,50,100,1000]}\n",
    "xgb = RandomizedSearchCV(XGBClassifier(),params,\n",
    "                  cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=3),\n",
    "                              scoring=make_scorer(pos_f1))\n",
    "xgb.fit(x_train.iloc[:,:], y_train[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1337    0]\n",
      " [  50  133]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      1337\n",
      "           1       1.00      0.73      0.84       183\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1520\n",
      "   macro avg       0.98      0.86      0.91      1520\n",
      "weighted avg       0.97      0.97      0.96      1520\n",
      "\n",
      "[[861   1]\n",
      " [152   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92       862\n",
      "           1       0.00      0.00      0.00       152\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      1014\n",
      "   macro avg       0.42      0.50      0.46      1014\n",
      "weighted avg       0.72      0.85      0.78      1014\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mc['xgb']=pd.Series(\n",
    "    confusion_matrix(y_test, xgb.predict(x_test)).ravel())\n",
    "print(confusion_matrix(y_train, xgb.predict(x_train)))  \n",
    "print(classification_report(y_train, xgb.predict(x_train))) \n",
    "\n",
    "print(confusion_matrix(y_test, xgb.predict(x_test)))  \n",
    "print(classification_report(y_test, xgb.predict(x_test))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tvx, x_test1, tvy, y_test1 = train_test_split(X.drop('high_bp_1',axis=1), \n",
    "                                                    X['high_bp_1'], \n",
    "                                                    test_size = 0.25)\n",
    "tx, vx, ty, vy = train_test_split(tvx, tvy,test_size = 0.33333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "eval_metric = [\"auc\",\"error\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.549626\tvalidation_0-error:0.119874\n",
      "Multiple eval metrics have been passed: 'validation_0-error' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-error hasn't improved in 10 rounds.\n",
      "[1]\tvalidation_0-auc:0.572587\tvalidation_0-error:0.116719\n",
      "[2]\tvalidation_0-auc:0.5809\tvalidation_0-error:0.118297\n",
      "[3]\tvalidation_0-auc:0.578511\tvalidation_0-error:0.118297\n",
      "[4]\tvalidation_0-auc:0.58388\tvalidation_0-error:0.116719\n",
      "[5]\tvalidation_0-auc:0.609049\tvalidation_0-error:0.116719\n",
      "[6]\tvalidation_0-auc:0.618074\tvalidation_0-error:0.116719\n",
      "[7]\tvalidation_0-auc:0.62319\tvalidation_0-error:0.115142\n",
      "[8]\tvalidation_0-auc:0.625688\tvalidation_0-error:0.115142\n",
      "[9]\tvalidation_0-auc:0.634713\tvalidation_0-error:0.115142\n",
      "[10]\tvalidation_0-auc:0.629139\tvalidation_0-error:0.115142\n",
      "[11]\tvalidation_0-auc:0.651038\tvalidation_0-error:0.115142\n",
      "[12]\tvalidation_0-auc:0.640999\tvalidation_0-error:0.115142\n",
      "[13]\tvalidation_0-auc:0.647092\tvalidation_0-error:0.115142\n",
      "[14]\tvalidation_0-auc:0.647623\tvalidation_0-error:0.115142\n",
      "[15]\tvalidation_0-auc:0.644703\tvalidation_0-error:0.116719\n",
      "[16]\tvalidation_0-auc:0.63972\tvalidation_0-error:0.116719\n",
      "[17]\tvalidation_0-auc:0.646706\tvalidation_0-error:0.116719\n",
      "Stopping. Best iteration:\n",
      "[7]\tvalidation_0-auc:0.62319\tvalidation_0-error:0.115142\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.fit(tx,ty,eval_set=[(vx,vy)],eval_metric=eval_metric,early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    790\n",
      "1     72\n",
      "2    133\n",
      "3     19\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(mc['svm_sig'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "714"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/alex/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model.add(tf.keras.layers.Dense(100,activation=tf.nn.relu,\n",
    "                                input_dim=x_train.shape[1]))\n",
    "model.add(tf.keras.layers.Dense(100,activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(1,activation=tf.nn.sigmoid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/alex/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "1520/1520 [==============================] - 1s 341us/sample - loss: 0.4146 - acc: 0.8566\n",
      "Epoch 2/10\n",
      "1520/1520 [==============================] - 0s 123us/sample - loss: 0.3254 - acc: 0.8803\n",
      "Epoch 3/10\n",
      "1520/1520 [==============================] - 0s 100us/sample - loss: 0.2789 - acc: 0.8842\n",
      "Epoch 4/10\n",
      "1520/1520 [==============================] - 0s 98us/sample - loss: 0.2189 - acc: 0.9046\n",
      "Epoch 5/10\n",
      "1520/1520 [==============================] - 0s 93us/sample - loss: 0.1447 - acc: 0.9454\n",
      "Epoch 6/10\n",
      "1520/1520 [==============================] - 0s 99us/sample - loss: 0.0753 - acc: 0.9822\n",
      "Epoch 7/10\n",
      "1520/1520 [==============================] - 0s 103us/sample - loss: 0.0303 - acc: 0.9980\n",
      "Epoch 8/10\n",
      "1520/1520 [==============================] - 0s 94us/sample - loss: 0.0124 - acc: 0.9993\n",
      "Epoch 9/10\n",
      "1520/1520 [==============================] - 0s 98us/sample - loss: 0.0051 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "1520/1520 [==============================] - 0s 102us/sample - loss: 0.0026 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a2df3cef0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train.values,y_train.values,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [False],\n",
       "       [False],\n",
       "       ...,\n",
       "       [False],\n",
       "       [False],\n",
       "       [False]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test)>.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[983 354]\n",
      " [  0 183]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.74      0.85      1337\n",
      "           1       0.34      1.00      0.51       183\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      1520\n",
      "   macro avg       0.67      0.87      0.68      1520\n",
      "weighted avg       0.92      0.77      0.81      1520\n",
      "\n",
      "[[465 397]\n",
      " [ 78  74]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.54      0.66       862\n",
      "           1       0.16      0.49      0.24       152\n",
      "\n",
      "   micro avg       0.53      0.53      0.53      1014\n",
      "   macro avg       0.51      0.51      0.45      1014\n",
      "weighted avg       0.75      0.53      0.60      1014\n",
      "\n"
     ]
    }
   ],
   "source": [
    "thresh = .001\n",
    "print(confusion_matrix(y_train, \n",
    "                      [1 if i>thresh else 0 for i in model.predict(x_train)]))  \n",
    "print(classification_report(y_train, \n",
    "                           [1 if i>thresh else 0 for i in model.predict(x_train)])) \n",
    "\n",
    "print(confusion_matrix(y_test, \n",
    "                      [1 if i>thresh else 0 for i in model.predict(x_test)]) ) \n",
    "print(classification_report(y_test, \n",
    "                            [1 if i>thresh else 0 for i in model.predict(x_test)])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1 if i>.5 else 0 for i in model.predict(x_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pos_f1_1(y_pred, y_true):\n",
    "    y_true = [1 if i>.5 else 0 for i in y_true.get_label()]\n",
    "    y_pred = [1 if i>.5 else 0 for i in y_pred]\n",
    "    ty1 = [True if i==1 else False for i in y_true]\n",
    "    tp1 = [True if i==1 else False for i in y_pred]\n",
    "    rec = sum(np.equal(np.array(y_pred)[ty1],np.array(y_true)[ty1]))/sum(ty1)\n",
    "    if sum(tp1)==0:\n",
    "        prec=0\n",
    "    else:\n",
    "        prec = sum(np.equal(np.array(y_true)[tp1],np.array(y_pred)[tp1]))/sum(tp1)\n",
    "    if rec+prec == 0:\n",
    "        out = 0\n",
    "    else:\n",
    "        out = 2*((rec*prec)/(rec+prec))\n",
    "    return 'f1', -1*out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def misclassified(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    custom evaluation metric for xgboost, the metric\n",
    "    counts the number of misclassified examples assuming \n",
    "    that classes with p>0.5 are positive\n",
    "    \"\"\"\n",
    "    labels = y_true.get_label() # obtain true labels\n",
    "    preds = y_pred > 0.5 # obtain predicted values\n",
    "    return 'misclassified', np.sum(labels != preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pos_recall(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    ty1 = [True if i>.5 else False for i in y_true]\n",
    "    out = sum(np.equal(np.array(y_pred)[ty1],np.array(y_true)[ty1]))/sum(ty1)\n",
    "    return 'pr', 1-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pos_recall(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    y_pred = [1 if i >.5 else 0 for i in y_pred]\n",
    "    ty1 = [True if i==1 else False for i in y_true]\n",
    "    out = sum(np.equal(np.array(y_pred)[ty1],np.array(y_true)[ty1]))/sum(ty1)\n",
    "    return 'pr', 1-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import dlib\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Flatten, Dense, Activation, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs1 = np.random.choice(x_train[y_train==0].index,sum(y_train),replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  11,   13,   19,   20,   38,   45,   70,   72,   77,   78,   80,\n",
       "         81,   89,   94,   96,  101,  102,  112,  116,  125,  129,  130,\n",
       "        131,  132,  141,  145,  149,  152,  157,  172,  173,  182,  184,\n",
       "        185,  189,  193,  199,  212,  218,  222,  229,  231,  238,  242,\n",
       "        248,  251,  255,  273,  280,  290,  303,  304,  307,  319,  322,\n",
       "        330,  331,  337,  346,  348,  349,  376,  381,  385,  390,  395,\n",
       "        409,  413,  414,  417,  436,  442,  454,  470,  472,  479,  496,\n",
       "        498,  501,  524,  529,  534,  554,  557,  565,  569,  575,  577,\n",
       "        583,  586,  595,  604,  615,  616,  618,  629,  630,  638,  641,\n",
       "        653,  656,  658,  666,  675,  679,  687,  688,  689,  694,  711,\n",
       "        712,  715,  721,  730,  737,  739,  745,  748,  749,  751,  768,\n",
       "        790,  795,  803,  814,  816,  831,  833,  839,  853,  862,  871,\n",
       "        887,  894,  905,  907,  912,  917,  936,  942,  947,  951,  983,\n",
       "        987,  988,  991,  995, 1018, 1021, 1022, 1031, 1037, 1056, 1082,\n",
       "       1092, 1100, 1103, 1104, 1110, 1111, 1124, 1148, 1159, 1166, 1172,\n",
       "       1177, 1189, 1191, 1198, 1202, 1212, 1215, 1232, 1243, 1254, 1269,\n",
       "       1273, 1277, 1278, 1298, 1303, 1316, 1317, 1318, 1328, 1331, 1332,\n",
       "       1347, 1361, 1362, 1395, 1396, 1400, 1401, 1409, 1418, 1425, 1427,\n",
       "       1446, 1447, 1450, 1464, 1472, 1491, 1503, 1504, 1505, 1519, 1530,\n",
       "       1549, 1555, 1556, 1577, 1580, 1581, 1589, 1590, 1594, 1598, 1602,\n",
       "       1604, 1609, 1620, 1621, 1624, 1634, 1644, 1652, 1664, 1682, 1685,\n",
       "       1694, 1695, 1701, 1705, 1717, 1718, 1720, 1726, 1734, 1741, 1743,\n",
       "       1752, 1780, 1788, 1791, 1792, 1804, 1812, 1814, 1824, 1837, 1842,\n",
       "       1850, 1851, 1865, 1869, 1872, 1879, 1885, 1900, 1915, 1928, 1932,\n",
       "       1943, 1961, 1965, 1970, 1995, 2000, 2008, 2015, 2026])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.array(x_train[y_train==1].index)\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ind = np.concatenate((rs1,x1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(x_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt = np.array(y_train)[x_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = np.array(x_train)[x_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbp_array = keras.utils.to_categorical(yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xt0 = np.array(x_train[y_train==0].sample(sum(y_train))).reshape(sum(y_train),\n",
    "                                                           x_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(546, 714)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/alex/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "optim = keras.optimizers.SGD(lr=0.1, momentum=0.9, decay=0.00001, \n",
    "                             nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(1000,activation='relu', input_shape=(20,)))\n",
    "model.add(Dense(2000,activation='relu'))\n",
    "model.add(Dense(2000,activation='relu'))\n",
    "model.add(Dropout(.5,seed=1234))\n",
    "model.add(Dense(1000,activation='relu'))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "\n",
    "model.add(Dense(2,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optim,loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished loop 1\n",
      "finished loop 2\n",
      "finished loop 3\n",
      "finished loop 4\n",
      "finished loop 5\n",
      "finished loop 6\n",
      "finished loop 7\n",
      "finished loop 8\n",
      "finished loop 9\n",
      "finished loop 10\n",
      "finished loop 11\n",
      "finished loop 12\n",
      "finished loop 13\n",
      "finished loop 14\n",
      "finished loop 15\n",
      "finished loop 16\n",
      "finished loop 17\n",
      "finished loop 18\n",
      "finished loop 19\n",
      "finished loop 20\n",
      "finished loop 21\n",
      "finished loop 22\n",
      "finished loop 23\n",
      "finished loop 24\n",
      "finished loop 25\n",
      "finished loop 26\n",
      "finished loop 27\n",
      "finished loop 28\n",
      "finished loop 29\n",
      "finished loop 30\n",
      "finished loop 31\n",
      "finished loop 32\n",
      "finished loop 33\n",
      "finished loop 34\n",
      "finished loop 35\n",
      "finished loop 36\n",
      "finished loop 37\n",
      "finished loop 38\n",
      "finished loop 39\n",
      "finished loop 40\n",
      "finished loop 41\n",
      "finished loop 42\n",
      "finished loop 43\n",
      "finished loop 44\n",
      "finished loop 45\n",
      "finished loop 46\n",
      "finished loop 47\n",
      "finished loop 48\n",
      "finished loop 49\n",
      "finished loop 50\n",
      "finished loop 51\n",
      "finished loop 52\n",
      "finished loop 53\n",
      "finished loop 54\n",
      "finished loop 55\n",
      "finished loop 56\n",
      "finished loop 57\n",
      "finished loop 58\n",
      "finished loop 59\n",
      "finished loop 60\n",
      "finished loop 61\n",
      "finished loop 62\n",
      "finished loop 63\n",
      "finished loop 64\n",
      "finished loop 65\n",
      "finished loop 66\n",
      "finished loop 67\n",
      "finished loop 68\n",
      "finished loop 69\n",
      "finished loop 70\n",
      "finished loop 71\n",
      "finished loop 72\n",
      "finished loop 73\n",
      "finished loop 74\n",
      "finished loop 75\n",
      "finished loop 76\n",
      "finished loop 77\n",
      "finished loop 78\n",
      "finished loop 79\n",
      "finished loop 80\n",
      "finished loop 81\n",
      "finished loop 82\n",
      "finished loop 83\n",
      "finished loop 84\n",
      "finished loop 85\n",
      "finished loop 86\n",
      "finished loop 87\n",
      "finished loop 88\n",
      "finished loop 89\n",
      "finished loop 90\n",
      "finished loop 91\n",
      "finished loop 92\n",
      "finished loop 93\n",
      "finished loop 94\n",
      "finished loop 95\n",
      "finished loop 96\n",
      "finished loop 97\n",
      "finished loop 98\n",
      "finished loop 99\n",
      "finished loop 100\n",
      "finished loop 101\n",
      "finished loop 102\n",
      "finished loop 103\n",
      "finished loop 104\n",
      "finished loop 105\n",
      "finished loop 106\n",
      "finished loop 107\n",
      "finished loop 108\n",
      "finished loop 109\n",
      "finished loop 110\n",
      "finished loop 111\n",
      "finished loop 112\n",
      "finished loop 113\n",
      "finished loop 114\n",
      "finished loop 115\n",
      "finished loop 116\n",
      "finished loop 117\n",
      "finished loop 118\n",
      "finished loop 119\n",
      "finished loop 120\n",
      "finished loop 121\n",
      "finished loop 122\n",
      "finished loop 123\n",
      "finished loop 124\n",
      "finished loop 125\n",
      "finished loop 126\n",
      "finished loop 127\n",
      "finished loop 128\n",
      "finished loop 129\n",
      "finished loop 130\n",
      "finished loop 131\n",
      "finished loop 132\n",
      "finished loop 133\n",
      "finished loop 134\n",
      "finished loop 135\n",
      "finished loop 136\n",
      "finished loop 137\n",
      "finished loop 138\n",
      "finished loop 139\n",
      "finished loop 140\n",
      "finished loop 141\n",
      "finished loop 142\n",
      "finished loop 143\n",
      "finished loop 144\n",
      "finished loop 145\n",
      "finished loop 146\n",
      "finished loop 147\n",
      "finished loop 148\n",
      "finished loop 149\n",
      "finished loop 150\n",
      "finished loop 151\n",
      "finished loop 152\n",
      "finished loop 153\n",
      "finished loop 154\n",
      "finished loop 155\n",
      "finished loop 156\n",
      "finished loop 157\n",
      "finished loop 158\n",
      "finished loop 159\n",
      "finished loop 160\n",
      "finished loop 161\n",
      "finished loop 162\n",
      "finished loop 163\n",
      "finished loop 164\n",
      "finished loop 165\n",
      "finished loop 166\n",
      "finished loop 167\n",
      "finished loop 168\n",
      "finished loop 169\n",
      "finished loop 170\n",
      "finished loop 171\n",
      "finished loop 172\n",
      "finished loop 173\n",
      "finished loop 174\n",
      "finished loop 175\n",
      "finished loop 176\n",
      "finished loop 177\n",
      "finished loop 178\n",
      "finished loop 179\n",
      "finished loop 180\n",
      "finished loop 181\n",
      "finished loop 182\n",
      "finished loop 183\n",
      "finished loop 184\n",
      "finished loop 185\n",
      "finished loop 186\n",
      "finished loop 187\n",
      "finished loop 188\n",
      "finished loop 189\n",
      "finished loop 190\n",
      "finished loop 191\n",
      "finished loop 192\n",
      "finished loop 193\n",
      "finished loop 194\n",
      "finished loop 195\n",
      "finished loop 196\n",
      "finished loop 197\n",
      "finished loop 198\n",
      "finished loop 199\n",
      "finished loop 200\n",
      "finished loop 201\n",
      "finished loop 202\n",
      "finished loop 203\n",
      "finished loop 204\n",
      "finished loop 205\n",
      "finished loop 206\n",
      "finished loop 207\n",
      "finished loop 208\n",
      "finished loop 209\n",
      "finished loop 210\n",
      "finished loop 211\n",
      "finished loop 212\n",
      "finished loop 213\n",
      "finished loop 214\n",
      "finished loop 215\n",
      "finished loop 216\n",
      "finished loop 217\n",
      "finished loop 218\n",
      "finished loop 219\n",
      "finished loop 220\n",
      "finished loop 221\n",
      "finished loop 222\n",
      "finished loop 223\n",
      "finished loop 224\n",
      "finished loop 225\n",
      "finished loop 226\n",
      "finished loop 227\n",
      "finished loop 228\n",
      "finished loop 229\n",
      "finished loop 230\n",
      "finished loop 231\n",
      "finished loop 232\n",
      "finished loop 233\n",
      "finished loop 234\n",
      "finished loop 235\n",
      "finished loop 236\n",
      "finished loop 237\n",
      "finished loop 238\n",
      "finished loop 239\n",
      "finished loop 240\n",
      "finished loop 241\n",
      "finished loop 242\n",
      "finished loop 243\n",
      "finished loop 244\n",
      "finished loop 245\n",
      "finished loop 246\n",
      "finished loop 247\n",
      "finished loop 248\n",
      "finished loop 249\n",
      "finished loop 250\n",
      "finished loop 251\n",
      "finished loop 252\n",
      "finished loop 253\n",
      "finished loop 254\n",
      "finished loop 255\n",
      "finished loop 256\n",
      "finished loop 257\n",
      "finished loop 258\n",
      "finished loop 259\n",
      "finished loop 260\n",
      "finished loop 261\n",
      "finished loop 262\n",
      "finished loop 263\n",
      "finished loop 264\n",
      "finished loop 265\n",
      "finished loop 266\n",
      "finished loop 267\n",
      "finished loop 268\n",
      "finished loop 269\n",
      "finished loop 270\n",
      "finished loop 271\n",
      "finished loop 272\n",
      "finished loop 273\n",
      "finished loop 274\n",
      "finished loop 275\n",
      "finished loop 276\n",
      "finished loop 277\n",
      "finished loop 278\n",
      "finished loop 279\n",
      "finished loop 280\n",
      "finished loop 281\n",
      "finished loop 282\n",
      "finished loop 283\n",
      "finished loop 284\n",
      "finished loop 285\n",
      "finished loop 286\n",
      "finished loop 287\n",
      "finished loop 288\n",
      "finished loop 289\n",
      "finished loop 290\n",
      "finished loop 291\n",
      "finished loop 292\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-aab2f6db70db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mloopx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     history = model.fit(loopx,hbp_array,epochs=10,validation_split=.2,\n\u001b[0;32m---> 23\u001b[0;31m                         verbose=0)\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mbad_ac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m.65\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'finished loop '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2696\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2697\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_make_callable_from_options'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2699\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                     \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "npred = 20\n",
    "bad_ac = 0\n",
    "loop=0\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(npred,activation='relu', input_shape=(npred,)))\n",
    "model.add(Dense(npred*2,activation='relu'))\n",
    "model.add(Dropout(.5,seed=1234))\n",
    "model.add(Dense(npred*3,activation='relu'))\n",
    "model.add(Dense(npred*2,activation='relu'))\n",
    "model.add(Dense(npred,activation='relu'))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(optim,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "backm = model\n",
    "while bad_ac==0:\n",
    "    loop = loop +1\n",
    "    model = backm\n",
    "    \n",
    "    \n",
    "    loopx = xt[:,np.random.choice(range(xt.shape[1]),npred,replace=False)]\n",
    "    history = model.fit(loopx,hbp_array,epochs=10,validation_split=.2,\n",
    "                        verbose=0)\n",
    "    bad_ac = sum([True if i > .65 else False for i in history.history['val_acc']])\n",
    "    print('finished loop '+str(loop))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 436 samples, validate on 110 samples\n",
      "Epoch 1/50\n",
      "436/436 [==============================] - 9s 21ms/step - loss: 0.6928 - acc: 0.5252 - val_loss: 0.6683 - val_acc: 0.5818\n",
      "Epoch 2/50\n",
      "436/436 [==============================] - 0s 454us/step - loss: 0.6666 - acc: 0.5505 - val_loss: 0.6599 - val_acc: 0.6182\n",
      "Epoch 3/50\n",
      "436/436 [==============================] - 0s 437us/step - loss: 0.5974 - acc: 0.6812 - val_loss: 0.6869 - val_acc: 0.6455\n",
      "Epoch 4/50\n",
      "436/436 [==============================] - 0s 482us/step - loss: 0.5659 - acc: 0.7087 - val_loss: 0.6897 - val_acc: 0.5727\n",
      "Epoch 5/50\n",
      "436/436 [==============================] - 0s 365us/step - loss: 0.4670 - acc: 0.7959 - val_loss: 0.9723 - val_acc: 0.5727\n",
      "Epoch 6/50\n",
      "436/436 [==============================] - 0s 434us/step - loss: 0.4181 - acc: 0.8073 - val_loss: 1.3564 - val_acc: 0.5636\n",
      "Epoch 7/50\n",
      "436/436 [==============================] - 0s 424us/step - loss: 0.3433 - acc: 0.8417 - val_loss: 1.0229 - val_acc: 0.5636\n",
      "Epoch 8/50\n",
      "436/436 [==============================] - 0s 427us/step - loss: 0.1483 - acc: 0.9495 - val_loss: 1.9700 - val_acc: 0.5818\n",
      "Epoch 9/50\n",
      "436/436 [==============================] - 0s 504us/step - loss: 0.1943 - acc: 0.9289 - val_loss: 1.4094 - val_acc: 0.6000\n",
      "Epoch 10/50\n",
      "436/436 [==============================] - 0s 442us/step - loss: 0.2331 - acc: 0.8968 - val_loss: 1.4640 - val_acc: 0.5364\n",
      "Epoch 11/50\n",
      "436/436 [==============================] - 0s 475us/step - loss: 0.1856 - acc: 0.9404 - val_loss: 1.6853 - val_acc: 0.5818\n",
      "Epoch 12/50\n",
      "436/436 [==============================] - 0s 597us/step - loss: 0.1037 - acc: 0.9702 - val_loss: 2.3236 - val_acc: 0.5455\n",
      "Epoch 13/50\n",
      "436/436 [==============================] - 0s 462us/step - loss: 0.0754 - acc: 0.9725 - val_loss: 1.9373 - val_acc: 0.5818\n",
      "Epoch 14/50\n",
      "436/436 [==============================] - 0s 519us/step - loss: 0.0757 - acc: 0.9771 - val_loss: 1.9356 - val_acc: 0.6273\n",
      "Epoch 15/50\n",
      "436/436 [==============================] - 0s 524us/step - loss: 0.0612 - acc: 0.9771 - val_loss: 2.2096 - val_acc: 0.5636\n",
      "Epoch 16/50\n",
      "436/436 [==============================] - 0s 520us/step - loss: 0.0819 - acc: 0.9862 - val_loss: 2.2901 - val_acc: 0.6091\n",
      "Epoch 17/50\n",
      "436/436 [==============================] - 0s 472us/step - loss: 0.0466 - acc: 0.9885 - val_loss: 1.9218 - val_acc: 0.6091\n",
      "Epoch 18/50\n",
      "436/436 [==============================] - 0s 443us/step - loss: 0.0335 - acc: 0.9885 - val_loss: 2.2196 - val_acc: 0.5818\n",
      "Epoch 19/50\n",
      "436/436 [==============================] - 0s 532us/step - loss: 0.0119 - acc: 0.9954 - val_loss: 2.8722 - val_acc: 0.5636\n",
      "Epoch 20/50\n",
      "436/436 [==============================] - 0s 533us/step - loss: 0.0180 - acc: 0.9954 - val_loss: 3.0067 - val_acc: 0.5818\n",
      "Epoch 21/50\n",
      "436/436 [==============================] - 0s 511us/step - loss: 0.0132 - acc: 0.9931 - val_loss: 3.1062 - val_acc: 0.5727\n",
      "Epoch 22/50\n",
      "436/436 [==============================] - 0s 459us/step - loss: 0.0109 - acc: 0.9977 - val_loss: 3.1276 - val_acc: 0.5818\n",
      "Epoch 23/50\n",
      "436/436 [==============================] - 0s 442us/step - loss: 0.0521 - acc: 0.9817 - val_loss: 2.4499 - val_acc: 0.6000\n",
      "Epoch 24/50\n",
      "436/436 [==============================] - 0s 342us/step - loss: 0.0752 - acc: 0.9771 - val_loss: 2.4996 - val_acc: 0.5727\n",
      "Epoch 25/50\n",
      "436/436 [==============================] - 0s 379us/step - loss: 0.0340 - acc: 0.9885 - val_loss: 2.3449 - val_acc: 0.5727\n",
      "Epoch 26/50\n",
      "436/436 [==============================] - 0s 483us/step - loss: 0.0114 - acc: 0.9977 - val_loss: 2.9887 - val_acc: 0.5455\n",
      "Epoch 27/50\n",
      "436/436 [==============================] - 0s 491us/step - loss: 0.0081 - acc: 0.9977 - val_loss: 3.1805 - val_acc: 0.5909\n",
      "Epoch 28/50\n",
      "436/436 [==============================] - 0s 474us/step - loss: 0.0097 - acc: 0.9977 - val_loss: 3.9174 - val_acc: 0.5364\n",
      "Epoch 29/50\n",
      "436/436 [==============================] - 0s 531us/step - loss: 0.0477 - acc: 0.9862 - val_loss: 2.9968 - val_acc: 0.5909\n",
      "Epoch 30/50\n",
      "436/436 [==============================] - 0s 474us/step - loss: 0.0613 - acc: 0.9885 - val_loss: 2.7050 - val_acc: 0.5727\n",
      "Epoch 31/50\n",
      "436/436 [==============================] - 0s 480us/step - loss: 0.0234 - acc: 0.9954 - val_loss: 2.5997 - val_acc: 0.5636\n",
      "Epoch 32/50\n",
      "436/436 [==============================] - 0s 491us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 2.9104 - val_acc: 0.6000\n",
      "Epoch 33/50\n",
      "436/436 [==============================] - 0s 510us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 3.1404 - val_acc: 0.6000\n",
      "Epoch 34/50\n",
      "436/436 [==============================] - 0s 444us/step - loss: 9.1324e-04 - acc: 1.0000 - val_loss: 3.3018 - val_acc: 0.6091\n",
      "Epoch 35/50\n",
      "436/436 [==============================] - 0s 558us/step - loss: 6.5642e-04 - acc: 1.0000 - val_loss: 3.4359 - val_acc: 0.6091\n",
      "Epoch 36/50\n",
      "436/436 [==============================] - 0s 468us/step - loss: 3.4662e-04 - acc: 1.0000 - val_loss: 3.5137 - val_acc: 0.6091\n",
      "Epoch 37/50\n",
      "436/436 [==============================] - 0s 513us/step - loss: 2.9462e-04 - acc: 1.0000 - val_loss: 3.5589 - val_acc: 0.6091\n",
      "Epoch 38/50\n",
      "436/436 [==============================] - 0s 519us/step - loss: 2.0728e-04 - acc: 1.0000 - val_loss: 3.5889 - val_acc: 0.6091\n",
      "Epoch 39/50\n",
      "436/436 [==============================] - 0s 578us/step - loss: 3.7310e-04 - acc: 1.0000 - val_loss: 3.6188 - val_acc: 0.6091\n",
      "Epoch 40/50\n",
      "436/436 [==============================] - 0s 544us/step - loss: 1.1154e-04 - acc: 1.0000 - val_loss: 3.6392 - val_acc: 0.6091\n",
      "Epoch 41/50\n",
      "436/436 [==============================] - 0s 535us/step - loss: 9.2821e-05 - acc: 1.0000 - val_loss: 3.6553 - val_acc: 0.6091\n",
      "Epoch 42/50\n",
      "436/436 [==============================] - 0s 522us/step - loss: 2.0817e-04 - acc: 1.0000 - val_loss: 3.6748 - val_acc: 0.6091\n",
      "Epoch 43/50\n",
      "436/436 [==============================] - 0s 392us/step - loss: 1.1519e-04 - acc: 1.0000 - val_loss: 3.6997 - val_acc: 0.6091\n",
      "Epoch 44/50\n",
      "436/436 [==============================] - 0s 461us/step - loss: 6.1479e-05 - acc: 1.0000 - val_loss: 3.7137 - val_acc: 0.6091\n",
      "Epoch 45/50\n",
      "436/436 [==============================] - 0s 493us/step - loss: 1.0798e-04 - acc: 1.0000 - val_loss: 3.7271 - val_acc: 0.6091\n",
      "Epoch 46/50\n",
      "436/436 [==============================] - 0s 443us/step - loss: 8.9726e-05 - acc: 1.0000 - val_loss: 3.7408 - val_acc: 0.6091\n",
      "Epoch 47/50\n",
      "436/436 [==============================] - 0s 546us/step - loss: 7.9538e-05 - acc: 1.0000 - val_loss: 3.7530 - val_acc: 0.6091\n",
      "Epoch 48/50\n",
      "436/436 [==============================] - 0s 501us/step - loss: 2.9431e-04 - acc: 1.0000 - val_loss: 3.7690 - val_acc: 0.6091\n",
      "Epoch 49/50\n",
      "436/436 [==============================] - 0s 485us/step - loss: 9.1253e-05 - acc: 1.0000 - val_loss: 3.7982 - val_acc: 0.6000\n",
      "Epoch 50/50\n",
      "436/436 [==============================] - 0s 468us/step - loss: 4.4400e-05 - acc: 1.0000 - val_loss: 3.8093 - val_acc: 0.6000\n"
     ]
    }
   ],
   "source": [
    "npred = sum(hc)\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(npred,activation='relu', input_shape=(npred,)))\n",
    "model.add(Dense(npred*2,activation='relu'))\n",
    "model.add(Dropout(.5,seed=1234))\n",
    "model.add(Dense(npred*3,activation='relu'))\n",
    "model.add(Dense(npred*2,activation='relu'))\n",
    "model.add(Dense(npred,activation='relu'))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(optim,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(xt[:,hc],\n",
    "                    hbp_array,epochs=50,\n",
    "                    validation_split=.2,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in greater\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors 1 0.5091743113797739 0.5000000010837208\n",
      "Predictors 2 0.5848623853211009 0.5909090909090909\n",
      "Predictors 3 0.6330275234826114 0.6363636374473571\n",
      "Predictors 4 0.5825688084331128 0.581818180734461\n",
      "Predictors 5 0.6192660561395348 0.6000000032511624\n",
      "Predictors 6 0.6261467889908257 0.6545454556291753\n",
      "Predictors 7 0.6766055034934928 0.6909090898253701\n",
      "Predictors 8 0.6674311915668872 0.7000000032511624\n",
      "Predictors 9 0.692660549911884 0.6454545443708246\n",
      "Predictors 10 0.711009173765095 0.6090909123420716\n",
      "Predictors 11 0.6972477058751867 0.6545454567128962\n",
      "Predictors 12 0.6926605510055472 0.6363636396147988\n",
      "Predictors 13 0.711009173765095 0.654545457796617\n",
      "Predictors 14 0.6857798170605931 0.6272727305238898\n",
      "Predictors 15 0.6857798176074247 0.6272727305238898\n",
      "Predictors 16 0.7224770636733518 0.5818181850693442\n",
      "Predictors 17 0.7201834873321953 0.6454545487057078\n",
      "Predictors 18 0.7408256875265629 0.6636363625526428\n",
      "Predictors 19 0.7362385321100917 0.6181818214329806\n",
      "Predictors 20 0.7385321106385747 0.6090909123420716\n",
      "Predictors 21 0.7522935790753146 0.6272727305238898\n",
      "Predictors 22 0.7431192660550459 0.6545454545454545\n",
      "Predictors 23 0.7155963291815661 0.6545454545454545\n",
      "Predictors 24 0.7775229363266482 0.654545457796617\n",
      "Predictors 25 0.7408256886202261 0.7272727261890065\n",
      "Predictors 26 0.8027522941248133 0.6363636396147988\n",
      "Predictors 27 0.7522935774348197 0.6272727305238898\n",
      "Predictors 28 0.7454128445835289 0.6454545487057078\n",
      "Predictors 29 0.7935779805577129 0.6636363658038053\n",
      "Predictors 30 0.7958715596330275 0.6454545487057078\n",
      "Predictors 31 0.7958715596330275 0.654545457796617\n",
      "Predictors 32 0.80275229303115 0.6272727261890064\n",
      "Predictors 33 0.7844036697247706 0.6454545487057078\n",
      "Predictors 34 0.8211009174311926 0.6454545487057078\n",
      "Predictors 35 0.82798165246981 0.663636366887526\n",
      "Predictors 36 0.8417431181723919 0.6454545487057078\n",
      "Predictors 37 0.8142201840330702 0.6545454534617338\n",
      "Predictors 38 0.8486238521173459 0.6181818170980974\n",
      "Predictors 39 0.846330276323021 0.6727272759784352\n",
      "Predictors 40 0.8922018343155537 0.6363636352799156\n",
      "Predictors 41 0.8692660561395348 0.6454545487057078\n",
      "Predictors 42 0.8738532099155111 0.6181818181818182\n",
      "Predictors 43 0.8555045871559633 0.6181818187236786\n",
      "Predictors 44 0.8669724781578834 0.6636363636363637\n",
      "Predictors 45 0.8784403658788139 0.654545457796617\n",
      "Predictors 46 0.8646788990825688 0.6181818203492598\n",
      "Predictors 47 0.9036697242238106 0.663636366887526\n",
      "Predictors 48 0.9105504576219331 0.6545454534617338\n",
      "Predictors 49 0.9013761473358224 0.6636363636363637\n",
      "Predictors 50 0.8922018337687221 0.7000000032511624\n",
      "Predictors 51 0.9288990825688074 0.6090909123420716\n",
      "Predictors 52 0.9334862385321101 0.6545454545454545\n",
      "Predictors 53 0.8967889902788565 0.6363636396147988\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-217-2d8d83ac591c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m                         \u001b[0mhbp_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                         verbose=0)\n\u001b[0m\u001b[1;32m     19\u001b[0m     print('Predictors '+str(i),\n\u001b[1;32m     20\u001b[0m           \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2669\u001b[0m                                 \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m                                 session)\n\u001b[0m\u001b[1;32m   2672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[0;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[1;32m   2621\u001b[0m             \u001b[0mcallable_opts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m         \u001b[0;31m# Create callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m         \u001b[0mcallable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2624\u001b[0m         \u001b[0;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m         \u001b[0;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[0;34m(self, callable_options)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     \"\"\"\n\u001b[1;32m   1470\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1471\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session, callable_options)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m           self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[0;32m-> 1425\u001b[0;31m               session._session, options_ptr, status)\n\u001b[0m\u001b[1;32m   1426\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lxt = xt[:,si[huh[si]>.05]]\n",
    "for i in np.arange(1,lxt.shape[1]):\n",
    "    npred=i\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(npred,activation='relu', input_shape=(i,)))\n",
    "    model.add(Dense(npred*2,activation='relu'))\n",
    "    model.add(Dropout(.5,seed=1234))\n",
    "    model.add(Dense(npred*3,activation='relu'))\n",
    "    model.add(Dense(npred*2,activation='relu'))\n",
    "    model.add(Dense(npred,activation='relu'))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.compile(optim,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(lxt[:,:i],\n",
    "                        hbp_array,epochs=20,\n",
    "                        validation_split=.2,\n",
    "                        verbose=0)\n",
    "    print('Predictors '+str(i),\n",
    "          max(history.history['acc']),\n",
    "          max(history.history['val_acc']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in greater\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 436 samples, validate on 110 samples\n",
      "Epoch 1/10\n",
      "436/436 [==============================] - 15s 35ms/step - loss: 0.6937 - acc: 0.5092 - val_loss: 0.6986 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "436/436 [==============================] - 0s 446us/step - loss: 0.6985 - acc: 0.5665 - val_loss: 0.6965 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "436/436 [==============================] - 0s 438us/step - loss: 0.6797 - acc: 0.5665 - val_loss: 0.6794 - val_acc: 0.5727\n",
      "Epoch 4/10\n",
      "436/436 [==============================] - 0s 391us/step - loss: 0.6790 - acc: 0.5780 - val_loss: 0.6882 - val_acc: 0.5091\n",
      "Epoch 5/10\n",
      "436/436 [==============================] - 0s 318us/step - loss: 0.6733 - acc: 0.5963 - val_loss: 0.7101 - val_acc: 0.5091\n",
      "Epoch 6/10\n",
      "436/436 [==============================] - 0s 447us/step - loss: 0.6659 - acc: 0.5963 - val_loss: 0.6888 - val_acc: 0.5273\n",
      "Epoch 7/10\n",
      "436/436 [==============================] - 0s 444us/step - loss: 0.6546 - acc: 0.6514 - val_loss: 0.6916 - val_acc: 0.5364\n",
      "Epoch 8/10\n",
      "436/436 [==============================] - 0s 441us/step - loss: 0.6614 - acc: 0.6422 - val_loss: 0.6834 - val_acc: 0.5455\n",
      "Epoch 9/10\n",
      "436/436 [==============================] - 0s 445us/step - loss: 0.6557 - acc: 0.6261 - val_loss: 0.6835 - val_acc: 0.5364\n",
      "Epoch 10/10\n",
      "436/436 [==============================] - 0s 454us/step - loss: 0.6605 - acc: 0.6032 - val_loss: 0.6680 - val_acc: 0.5545\n"
     ]
    }
   ],
   "source": [
    "lxt = xt[:,si[huh[si]>.05]]\n",
    "npred=25\n",
    "model = Sequential()\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(npred,activation='relu', input_shape=(npred,)))\n",
    "model.add(Dense(int(npred*1.5),activation='relu'))\n",
    "model.add(Dropout(.5,seed=1234))\n",
    "model.add(Dense(int(npred*2),activation='relu'))\n",
    "model.add(Dropout(.5,seed=1234))\n",
    "model.add(Dense(int(npred*2),activation='relu'))\n",
    "model.add(Dropout(.5,seed=1234))\n",
    "model.add(Dense(int(npred*1.5),activation='relu'))\n",
    "model.add(Dense(npred,activation='relu'))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(optim,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(lxt[:,:npred],\n",
    "                        hbp_array,epochs=10,\n",
    "                        validation_split=.2,\n",
    "                        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lxtest = xt[:,si[huh[si]>.05]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in greater\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.4395447e-02, 9.8560452e-01],\n",
       "       [2.2532420e-05, 9.9997747e-01],\n",
       "       [5.3453153e-01, 4.6546841e-01],\n",
       "       ...,\n",
       "       [9.7541410e-01, 2.4585905e-02],\n",
       "       [9.5666796e-01, 4.3332022e-02],\n",
       "       [3.4323926e-05, 9.9996567e-01]], dtype=float32)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array(x_test)[:,si[huh[si]>.05]][:,:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(lxt[:,:npred])\n",
    "model.predict_classes(lxt[:,:npred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in greater\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[356,  89],\n",
       "       [ 42,  20]])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,\n",
    "                 model.predict_classes(np.array(x_test)[:,si[huh[si]>.05]][:,:25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in greater\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/alex/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in greater\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[169 276]\n",
      " [ 14  48]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.38      0.54       445\n",
      "           1       0.15      0.77      0.25        62\n",
      "\n",
      "   micro avg       0.43      0.43      0.43       507\n",
      "   macro avg       0.54      0.58      0.39       507\n",
      "weighted avg       0.83      0.43      0.50       507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,\n",
    "                  np.array([i[1] for i in\\\n",
    "                               model.predict(np.array(x_test\\\n",
    "                                            )[:,si[huh[si]>.05]][:,:25])])>.4\n",
    "                      ))\n",
    "print(classification_report(y_test, \n",
    "                     np.array([i[1] for i in\\\n",
    "                               model.predict(np.array(x_test\\\n",
    "                                            )[:,si[huh[si]>.05]][:,:25])])>.4\n",
    "                           ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in greater\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, False, False, False,  True,  True, False,\n",
       "        True, False,  True, False,  True, False, False,  True,  True,\n",
       "       False, False, False, False, False, False, False,  True, False,\n",
       "        True,  True, False, False, False, False,  True, False,  True,\n",
       "        True,  True,  True, False, False,  True, False,  True, False,\n",
       "       False,  True, False, False,  True, False, False,  True,  True,\n",
       "       False,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True, False,  True, False,  True, False,  True,  True,  True,\n",
       "       False,  True,  True, False,  True,  True,  True, False, False,\n",
       "        True, False,  True,  True, False,  True, False,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True, False,\n",
       "        True, False, False,  True, False,  True,  True, False, False,\n",
       "        True, False, False, False, False, False, False,  True, False,\n",
       "        True, False, False,  True,  True,  True, False,  True, False,\n",
       "        True,  True, False,  True,  True,  True,  True, False, False,\n",
       "        True, False,  True, False, False,  True,  True, False,  True,\n",
       "       False,  True,  True,  True, False,  True,  True,  True, False,\n",
       "       False,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "       False,  True, False, False,  True,  True,  True,  True,  True,\n",
       "        True, False, False,  True,  True,  True,  True, False,  True,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "        True,  True, False, False, False, False,  True, False,  True,\n",
       "       False, False,  True,  True, False,  True, False,  True, False,\n",
       "       False, False, False, False,  True, False,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True, False, False,  True,\n",
       "       False, False, False, False,  True,  True, False, False,  True,\n",
       "       False,  True,  True, False, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False, False, False, False, False,\n",
       "       False,  True,  True,  True, False, False, False,  True, False,\n",
       "       False,  True, False,  True, False,  True, False, False,  True,\n",
       "        True,  True,  True, False, False, False,  True,  True,  True,\n",
       "        True,  True, False, False,  True,  True,  True, False, False,\n",
       "        True, False,  True,  True, False,  True,  True,  True, False,\n",
       "        True,  True, False,  True,  True, False, False,  True, False,\n",
       "       False, False,  True,  True,  True,  True, False,  True,  True,\n",
       "       False, False,  True, False, False,  True,  True, False, False,\n",
       "       False, False,  True,  True, False, False,  True,  True, False,\n",
       "        True, False, False,  True, False,  True,  True, False, False,\n",
       "        True,  True, False, False,  True, False, False, False,  True,\n",
       "        True,  True,  True,  True, False, False,  True, False,  True,\n",
       "       False,  True, False,  True, False,  True, False, False,  True,\n",
       "       False,  True, False, False, False,  True, False,  True,  True,\n",
       "        True,  True,  True,  True, False, False,  True, False, False,\n",
       "       False,  True,  True, False,  True, False, False, False,  True,\n",
       "        True, False,  True,  True, False,  True, False, False, False,\n",
       "        True,  True, False, False,  True,  True,  True, False, False,\n",
       "       False,  True, False,  True,  True, False, False, False, False,\n",
       "        True,  True, False, False,  True, False, False,  True, False,\n",
       "        True, False, False,  True,  True, False, False,  True, False,\n",
       "        True, False,  True,  True, False,  True,  True,  True,  True,\n",
       "       False, False,  True, False, False, False,  True, False,  True,\n",
       "        True,  True,  True, False, False,  True, False,  True, False,\n",
       "       False, False,  True,  True,  True, False, False, False,  True,\n",
       "        True,  True,  True,  True, False, False, False, False,  True,\n",
       "        True,  True, False, False,  True,  True, False,  True,  True,\n",
       "       False,  True, False, False, False, False,  True,  True, False,\n",
       "       False, False,  True])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([i[1] for i in model.predict(np.array(x_test)[:,si[huh[si]>.05]][:,:25])])>.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in greater\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([600, 191, 680, 455, 667, 121, 605, 125, 614, 570, 608, 699, 156,\n",
       "       703,  76, 252, 641,  40, 214, 102, 508,  36,  28, 461, 624, 216,\n",
       "       222,  19, 181, 179, 713, 108, 113, 422, 266, 712, 623, 645, 630,\n",
       "       656, 532, 437, 143,  37, 104, 308, 629, 606, 219, 116, 134, 377,\n",
       "        20,  62, 237, 381, 676,  49,  14, 615, 223,  11, 107, 613, 653,\n",
       "       710,  69, 210, 626, 136,  12,  77, 215, 411, 590, 132, 281,  96,\n",
       "        95, 602,   9, 545, 129, 239, 697, 255, 180, 414, 652, 112, 193,\n",
       "        50, 140, 154, 204, 464, 339, 604, 672, 155, 647, 704, 607, 370,\n",
       "        86,  39, 176, 379, 245, 671, 202, 421, 198, 144,  70, 566,   1,\n",
       "       261, 192, 640, 523, 334, 525, 575, 273, 292, 579, 497, 449, 501,\n",
       "       272, 285, 644, 188, 625, 435, 372,  99, 693, 679,  59, 382, 329,\n",
       "       635, 392, 567,  15, 408, 670, 123, 550, 325, 668, 322, 419, 665,\n",
       "        88,  16, 244, 122, 120, 360, 349, 170, 342, 340,  58, 695, 311,\n",
       "       638, 110, 128, 694, 469,  74, 393,  24, 688, 378,  29,   5, 424,\n",
       "       109, 601])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "si[huh[si]>.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "huh = np.abs([np.corrcoef(xt[:,i],yt)[0][1] for i in range(xt.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "si = huh.argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in greater\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/alex/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in greater\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([600, 191, 680, 455, 667, 121, 605, 125, 614, 570, 608, 699, 156,\n",
       "       703,  76, 252, 641,  40, 214, 102, 508,  36,  28, 461, 624, 216,\n",
       "       222,  19, 181, 179, 713, 108, 113, 422, 266, 712, 623, 645, 630,\n",
       "       656, 532, 437, 143,  37, 104, 308, 629, 606, 219, 116, 134, 377,\n",
       "        20,  62, 237, 381, 676,  49,  14, 615, 223,  11, 107, 613, 653,\n",
       "       710,  69, 210, 626, 136,  12,  77, 215, 411, 590, 132, 281,  96,\n",
       "        95, 602,   9, 545, 129, 239, 697, 255, 180, 414, 652, 112, 193,\n",
       "        50, 140, 154, 204, 464, 339, 604, 672, 155, 647, 704, 607, 370,\n",
       "        86,  39, 176, 379, 245, 671, 202, 421, 198, 144,  70, 566,   1,\n",
       "       261, 192, 640, 523, 334, 525, 575, 273, 292, 579, 497, 449, 501,\n",
       "       272, 285, 644, 188, 625, 435, 372,  99, 693, 679,  59, 382, 329,\n",
       "       635, 392, 567,  15, 408, 670, 123, 550, 325, 668, 322, 419, 665,\n",
       "        88,  16, 244, 122, 120, 360, 349, 170, 342, 340,  58, 695, 311,\n",
       "       638, 110, 128, 694, 469,  74, 393,  24, 688, 378,  29,   5, 424,\n",
       "       109, 601])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huh[si[huh[si]>.05]]\n",
    "si[huh[si]>.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False,  True, False,  True,  True,\n",
       "       False, False,  True, False, False, False,  True, False,  True,\n",
       "        True, False, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False,  True, False, False,  True,  True, False, False,\n",
       "        True, False, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False,  True, False,  True,  True,  True,  True, False, False,\n",
       "       False, False, False, False, False, False, False,  True, False,\n",
       "        True, False, False,  True, False,  True, False,  True,  True,\n",
       "        True, False, False, False, False, False, False,  True,  True,\n",
       "        True, False,  True,  True, False,  True,  True,  True,  True,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "        True, False,  True, False, False,  True,  True,  True, False,\n",
       "       False,  True, False, False,  True, False, False, False, False,\n",
       "       False, False, False, False, False, False,  True,  True,  True,\n",
       "       False, False,  True,  True, False, False, False, False, False,\n",
       "       False, False, False, False,  True, False, False,  True, False,\n",
       "       False, False,  True, False, False,  True, False, False,  True,\n",
       "        True, False, False,  True,  True, False,  True,  True, False,\n",
       "        True, False,  True,  True,  True,  True, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(np.abs([np.corrcoef(xt[:,i],yt)[0][1] for i in range(xt.shape[1])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
